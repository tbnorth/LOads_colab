{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ONOS-z05f6dp"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NW-Jh5er5kCq"
   },
   "source": [
    "```python\n",
    "# Install the PyDrive wrapper & import libraries.\n",
    "# This only needs to be done once per notebook.\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# Authenticate and create the PyDrive client.\n",
    "# This only needs to be done once per notebook.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "62UIOjgh6s1C",
    "outputId": "f78c92e2-dd8c-4566-9bd6-3379c04e2b71"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from google.colab import drive\n",
    "\n",
    "# drive.mount(\"/content/gdrive\")\n",
    "# ROOT = Path(\"/content/gdrive/My Drive/Colab Notebooks\")\n",
    "# BASE = ROOT.joinpath('LOads')\n",
    "BASE = Path(\"/content/gdrive/My Drive/Notebooks/LOads_colab\")\n",
    "BASE.mkdir(exist_ok=True)\n",
    "os.chdir(BASE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "colab_type": "code",
    "id": "MyaGn8iZEi3h",
    "outputId": "7a46b43d-08a3-4700-8eb7-487483ae4bb1"
   },
   "outputs": [],
   "source": [
    "!apt-get install libspatialindex-dev python3-rtree\n",
    "!pip install geopandas\n",
    "!ldconfig\n",
    "from rtree import index\n",
    "from rtree.index import Rtree\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas\n",
    "from collections import namedtuple\n",
    "from dateutil.parser import parse\n",
    "from datetime import timedelta\n",
    "import json\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J3E2_7srZVvJ"
   },
   "outputs": [],
   "source": [
    "def merge(left, right, on, cols):\n",
    "    for col in cols:\n",
    "        if col in left.columns:\n",
    "            left = left.drop(columns=col)\n",
    "    right = right.reset_index()\n",
    "    return left.merge(right[on+cols], on=on, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wheVzmbh73ny"
   },
   "outputs": [],
   "source": [
    "# uploaded get_wqdata.py to Colab Notebooks folder, now add to import path\n",
    "import sys\n",
    "if BASE not in sys.path:\n",
    "    sys.path.append(str(BASE))\n",
    "from get_wqdata import GetWQData, WQ_API_URL\n",
    "\n",
    "import get_wqdata\n",
    "from importlib import reload\n",
    "reload(get_wqdata)\n",
    "GetWQData = get_wqdata.GetWQData\n",
    "WQ_API_URL = get_wqdata.WQ_API_URL\n",
    "\n",
    "wq = GetWQData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Mm-pDqcgB0Y"
   },
   "source": [
    "# Explore dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cx15z3nOB5uJ"
   },
   "source": [
    "Want to work out which CharacteristicName values to request.  The WQ portal web interface says they come [from here](http://iaspub.epa.gov/sor_internet/registry/substreg/home/overview/home.do) but that's hard to search, so request all obs. in our bounding box and search that list instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 765
    },
    "colab_type": "code",
    "id": "gOM5hZUYCYxe",
    "outputId": "048e983f-1144-4350-8834-67966e870ca6"
   },
   "outputs": [],
   "source": [
    "query = dict(\n",
    "    _desc=\"All sample types\",\n",
    "    bBox=\"-80.0402,43.0646,-75.9697,44.355\",\n",
    "    siteType=[\"Aggregate surface-water-use\", \"Stream\"],\n",
    "    sampleMedia=[\"Water\", \"water\"],\n",
    "    # siteid=[\"USGS-04231600\"],\n",
    "    # startDateLo=\"01-01-2018\",\n",
    "    # startDateHi=\"02-01-2018\",\n",
    ")\n",
    "print(\"Reading data\")\n",
    "all_data = wq.get_data(query)  # gets path to .csv file\n",
    "d = pd.read_csv(all_data, low_memory=False)  # , nrows=10)\n",
    "print(\"Data read\")\n",
    "print(d.columns)\n",
    "po4 = set(d[\"CharacteristicName\"])\n",
    "keep = set()\n",
    "for term in 'po4', 'phosphor', 'srp', 'phosphate', 'flow', 'guage', 'cfs':\n",
    "    for cname in po4:\n",
    "        if term in cname.lower():\n",
    "            keep.add(cname)\n",
    "print(sorted(keep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fZxLuVimEBVF"
   },
   "source": [
    "Now make a sensible list from the values we found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VVO8fq7dBn55"
   },
   "outputs": [],
   "source": [
    "use_cache = {\n",
    "    'characteric_names': [\n",
    "        # 'O-Ethyl O-methyl S-propyl phosphorothioate',\n",
    "        # 'O-Ethyl S-methyl S-propyl phosphorodithioate',\n",
    "        # 'O-Ethyl S-propyl phosphorothioate',\n",
    "        # 'Organic phosphorus',\n",
    "        'Orthophosphate',\n",
    "        'Orthophosphate as P',\n",
    "        'Phosphate-phosphorus',\n",
    "        'Phosphate-phosphorus as P',\n",
    "        'Phosphorus',\n",
    "        'Stream flow, instantaneous',\n",
    "        'Stream flow, mean. daily',\n",
    "        # 'Tributyl phosphate',\n",
    "        # 'Triphenyl phosphate',\n",
    "        # 'Tris(1,3-dichloro-2-propyl)phosphate',\n",
    "        # 'Tris(2-butoxyethyl) phosphate',\n",
    "        # 'Tris(2-chloroethyl) phosphate',\n",
    "    ]\n",
    "}\n",
    "\n",
    "names = use_cache.get('characteric_names')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXAT9ESNFZXc"
   },
   "source": [
    "Now grab all the data we're interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "HLzhWPqJFQ7s",
    "outputId": "ad659483-bd3c-4099-8f6d-551600b70c03"
   },
   "outputs": [],
   "source": [
    "query = dict(\n",
    "    _desc=\"Target sample types\",\n",
    "    bBox=\"-80.0402,43.0646,-75.9697,44.355\",\n",
    "    siteType=[\"Aggregate surface-water-use\", \"Stream\"],\n",
    "    sampleMedia=[\"Water\", \"water\"],\n",
    "    characteristicName=names,\n",
    ")\n",
    "print(\"Reading data\")\n",
    "# first get the Site info. flavored response\n",
    "data = wq.get_data(query, type_='site')  # gets path to .csv file\n",
    "site = pd.read_csv(data)\n",
    "# print('\\n'.join(sorted(site.columns)))\n",
    "# then get the Result info.\n",
    "data = wq.get_data(query, type_='result')\n",
    "d = pd.read_csv(data, low_memory=False)\n",
    "d.columns = [i.replace('/', '_') for i in d.columns]\n",
    "# get rid of \"mg/l<space><space><space>\" units\n",
    "for col in ['ResultMeasure_MeasureUnitCode']:\n",
    "    d[col] = [str(i).strip() for i in d[col]]\n",
    "had = len(d)\n",
    "d = d.loc[d['ResultMeasure_MeasureUnitCode'] != 'nan', :]\n",
    "print(\"Lost %d for missing units\" % (had - len(d)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "PqUj6aVFGVV2",
    "outputId": "fc6329ab-91b4-43d3-c3df-6fc7eb310c10"
   },
   "outputs": [],
   "source": [
    "# find common columns\n",
    "common = set(site.columns).intersection(set(d.columns))\n",
    "print('\\n'.join(sorted(common)))\n",
    "# use this common column\n",
    "common = 'MonitoringLocationIdentifier'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hnho5UvugVTX"
   },
   "source": [
    "# Merge sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "UlnI67nVGmFe",
    "outputId": "b6d61fc4-ff4a-41d0-dca6-929f46df3857"
   },
   "outputs": [],
   "source": [
    "# create a list of unique sites\n",
    "locs = site.loc[:, [common, 'LatitudeMeasure', 'LongitudeMeasure']]\n",
    "locs.drop_duplicates(inplace=True)\n",
    "length = len(locs)\n",
    "length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ePo6lw_0HCh1"
   },
   "outputs": [],
   "source": [
    "# copy lat / lon to main data\n",
    "old_len = len(d)\n",
    "locs.reset_index(inplace=True, drop=True)\n",
    "d = d.merge(locs.loc[:, [common, 'LatitudeMeasure', 'LongitudeMeasure']], on=[common])\n",
    "assert len(d) == old_len\n",
    "assert sum(d.LatitudeMeasure == 0) == 0\n",
    "assert sum(np.isnan(d.LatitudeMeasure)) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G7n1oI1XI178"
   },
   "source": [
    "Data has different `MonitoringLocationIdentifier`s for the same coords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "OkFwnJS9IDWN",
    "outputId": "b1dcdf7d-ec2b-4937-e657-24dd83ec3612"
   },
   "outputs": [],
   "source": [
    "# generate a location based site field\n",
    "lonlat = d[['LongitudeMeasure', 'LatitudeMeasure']].drop_duplicates()\n",
    "dd = geopandas.GeoDataFrame(\n",
    "    lonlat,\n",
    "    geometry=geopandas.points_from_xy(\n",
    "        lonlat.LongitudeMeasure, lonlat.LatitudeMeasure\n",
    "    ),\n",
    "    crs = {'init' :'epsg:4326'},\n",
    ")\n",
    "dd['site'] = dd.LatitudeMeasure.astype(str)+dd.LongitudeMeasure.astype(str)\n",
    "dd.drop(columns=['LongitudeMeasure', 'LatitudeMeasure'], inplace=True)\n",
    "# 50 m buffer merges sites within 100 m of each other\n",
    "dd.geometry = dd.geometry.to_crs(5071).buffer(50)\n",
    "intersect = geopandas.overlay(dd, dd, how='identity')\n",
    "lint = len(intersect)\n",
    "intersect = intersect[intersect.site_2.astype(str) != 'nan']\n",
    "print(\"Dropped %d NaN intersections\" % (lint-len(intersect)))\n",
    "\n",
    "# make a dict of site: (set of intersecting sites)\n",
    "sets = {}\n",
    "for inter in intersect.itertuples():\n",
    "    site_1 = str(inter.site_1)\n",
    "    site_2 = str(inter.site_2)\n",
    "    sets.setdefault(site_1, set()).add(site_2)\n",
    "    sets.setdefault(site_2, set()).add(site_1)\n",
    "# pick the lowest (alphabetically) site as the name for intersecting sites\n",
    "sites = {min(i):i for i in sets.values()}\n",
    "# sites = {(k+\"_X\" if len(v) > 1 else k):v for k, v in sites.items()}\n",
    "[{k:v} for k,v in sites.items() if len(v) > 1]\n",
    "len(lonlat), len(sites)\n",
    "site = {}\n",
    "xtra = {}\n",
    "for k, v in sites.items():\n",
    "    for s in v:\n",
    "        site[s] = k\n",
    "        xtra[s] = len(v) > 1\n",
    "\n",
    "lu = d.loc[:, ['LongitudeMeasure', 'LatitudeMeasure', common]].drop_duplicates(\n",
    "    subset=['LongitudeMeasure', 'LatitudeMeasure'])\n",
    "#X lu = merge(lu, d, ['LongitudeMeasure', 'LatitudeMeasure'], [common])\n",
    "lu['sitehash'] = lu.LatitudeMeasure.astype(str)+lu.LongitudeMeasure.astype(str)\n",
    "sitename = {k:v for k,v in zip(lu.sitehash, lu[common])}\n",
    "lu['site'] = [sitename[site[i]]+('_X' if xtra[i] else '') for i in lu.sitehash]\n",
    "lu.drop_duplicates(inplace=True)\n",
    "\n",
    "d = merge(d, lu, ['LongitudeMeasure', 'LatitudeMeasure'], ['site'])\n",
    "assert len(d) == old_len, (old_len, len(d))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0pXzMa7wJ-3H"
   },
   "source": [
    "Now copy site info. into locs table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AgY12QBVKEtF"
   },
   "outputs": [],
   "source": [
    "nlocs = merge(locs, d, [common, 'LatitudeMeasure', 'LongitudeMeasure'], ['site'])\n",
    "locs_len = len(nlocs)\n",
    "nlocs.drop_duplicates(inplace=True, subset=['site', 'MonitoringLocationIdentifier'])\n",
    "assert len(nlocs) == length, (length, len(nlocs))\n",
    "locs = nlocs\n",
    "locs.to_csv(\"locs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mOItpESyYXtI"
   },
   "outputs": [],
   "source": [
    "for na in '', 'nan', 'NaN':\n",
    "    assert sum(locs['site'] == na) == 0, na\n",
    "    assert sum(d['site'] == na) == 0, na\n",
    "    assert sum(locs['site'] == na+na) == 0, na\n",
    "    assert sum(d['site'] == na+na) == 0, na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "1stEddL4jqzb",
    "outputId": "c848a340-f724-4426-ec71-a37c09ab4c5d"
   },
   "outputs": [],
   "source": [
    "print(\"%d locations\" % len(locs))\n",
    "distinct = locs.set_index([\"LatitudeMeasure\", \"LongitudeMeasure\"])\n",
    "print(\"%d distinct\" % len(distinct.index.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dXRyvPkVghYY"
   },
   "source": [
    "# Data to sampling events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "colab_type": "code",
    "id": "uVH5IJXWkhUN",
    "outputId": "e6e897de-edad-4823-835d-2d8c71d15744"
   },
   "outputs": [],
   "source": [
    "# tabulate units used for results\n",
    "d['ResultSampleFractionText'] = [\n",
    "    i if i != 'nan' else 'NA'\n",
    "    for i in d['ResultSampleFractionText'].astype(str)\n",
    "]\n",
    "\n",
    "res_types = (\n",
    "    d.groupby(\n",
    "        [\n",
    "            'CharacteristicName',\n",
    "            'ResultMeasure_MeasureUnitCode',\n",
    "            'ResultSampleFractionText',\n",
    "        ]\n",
    "    )\n",
    "    .count()\n",
    "    .iloc[:, 0]\n",
    ")\n",
    "print(res_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WvDaZ5-pkvVK"
   },
   "outputs": [],
   "source": [
    "duration = wq.db.get('duration', [])  # cache expensive calc. in wq.db\n",
    "if not duration:\n",
    "    for row in d.itertuples():\n",
    "        try:\n",
    "            t0 = parse(\n",
    "                \"%s %s\"\n",
    "                % (row.ActivityStartDate, row.ActivityStartTime_Time)\n",
    "            )\n",
    "            t1 = parse(\n",
    "                \"%s %s\" % (row.ActivityEndDate, row.ActivityEndTime_Time)\n",
    "            )\n",
    "            duration.append((t1 - t0).seconds)\n",
    "        except ValueError:\n",
    "            duration.append(0)\n",
    "    wq.db['duration'] = duration\n",
    "d['duration'] = duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qIbZBmsClx1c"
   },
   "source": [
    "Create an ID field for each sampling event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hYZXRm5jlp-_"
   },
   "outputs": [],
   "source": [
    "d['sampling'] = (\n",
    "    d['ActivityStartTime_Time'].astype(str)\n",
    "    + d['ActivityEndTime_Time'].astype(str)\n",
    "    + ' '\n",
    "    + d['ActivityStartDate'].astype(str)\n",
    "    + ' '\n",
    "    + d['ActivityEndDate'].astype(str)\n",
    "    + ' '\n",
    "    + d['LatitudeMeasure'].astype(str)\n",
    "    + ' '\n",
    "    + d['LongitudeMeasure'].astype(str)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Wv941RAmIeb"
   },
   "source": [
    "## Save output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HLjS3EC3mMfL"
   },
   "outputs": [],
   "source": [
    "d.to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7PqLby2umSla"
   },
   "source": [
    "Now we want to match P conc. data and flow data for each sampling event\n",
    "\n",
    "(start to refer to `d` as `data` from here on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "jHhBDk2nmh0q",
    "outputId": "0c8edb94-a6d8-4c43-89b9-9a618c2e572e"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data.csv\", low_memory=False)\n",
    "pdata = data.loc[:, ['sampling', 'ActivityStartDate', 'ActivityEndDate', 'site']]\n",
    "for na in '', 'nan', 'NaN':\n",
    "    assert sum(locs['site'] == na) == 0, na\n",
    "    assert sum(d['site'] == na) == 0, na\n",
    "    assert sum(pdata['site'] == na) == 0, na\n",
    "    assert sum(locs['site'] == na+na) == 0, na\n",
    "    assert sum(d['site'] == na+na) == 0, na\n",
    "    assert sum(pdata['site'] == na+na) == 0, na\n",
    "pdata.drop_duplicates(subset=['sampling'], inplace=True)\n",
    "pdata_len = len(pdata)\n",
    "print(\"%d into %d\" % (len(data), pdata_len))\n",
    "cname = 'CharacteristicName'\n",
    "cunit = 'ResultMeasure_MeasureUnitCode'\n",
    "vname = 'ResultMeasureValue'\n",
    "TP = namedtuple(\"ToProcess\", \"to cname unit trans\")\n",
    "po4_to_p = (30.97 + 4 * 16) / 30.97\n",
    "to_process = [\n",
    "    TP('flow', 'Stream flow, instantaneous', 'ft3/s', None),\n",
    "    TP(\n",
    "        'flow',\n",
    "        'Stream flow, instantaneous',\n",
    "        'm3/sec',\n",
    "        lambda x: x * 35.3147,\n",
    "    ),\n",
    "    TP('flow', 'Stream flow, mean. daily', 'ft3/s', None),\n",
    "    TP('conc', 'Phosphorus', 'mg/l as P', None),\n",
    "    TP('conc', 'Phosphorus', 'mg/l', None),\n",
    "    TP('conc', 'Phosphorus', 'ug/l', lambda x: 1000 * x),\n",
    "    TP('conc', 'Phosphorus', 'mg/l PO4', lambda x: x / po4_to_p),\n",
    "    TP('conc', 'Phosphate-phosphorus as P', 'mg/l', None),\n",
    "    TP('conc', 'Phosphate-phosphorus', 'mg/l', None),\n",
    "    TP('conc', 'Orthophosphate as P', 'mg/l', None),\n",
    "    TP('conc', 'Orthophosphate', 'mg/l as P', None),\n",
    "    TP('conc', 'Orthophosphate', 'mg/l asPO4', lambda x: x / po4_to_p),\n",
    "    TP('conc', 'Orthophosphate', 'mg/l', lambda x: x / po4_to_p),\n",
    "    # exclude as it's in bed sediment\n",
    "    # TP('conc', 'Phosphorus', 'mg/kg as P', None),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uPGAFGYinYDL"
   },
   "source": [
    "We're merging 82386 observations into 21877 flow + conc. records.  In a perfect world we'd have one flow and one conc. record for each sampling, but we have both redundant and mismatched (flow without conc. and visa versa) records, roughly 4:1 instead of 2:1.\n",
    "\n",
    "`to_process` is an **ordered** list of items where each item reocords the field we're trying to fill (`to`), the `CharacteristicName` (`cname`) of our prefered source field, our prefered `unit`, and any conversion (`trans`) necessary to use this source field / unit combination.\n",
    "\n",
    "Now we're going to fill the `to` fields (`flow`, `conc`) with the values selected using the list items, filling as many records with the first set of selected data, and only filling missing records with the next (less desireable) list item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "colab_type": "code",
    "id": "5eb6WyR0ozty",
    "outputId": "a722253f-91cd-49dc-ca80-53c15e97c090"
   },
   "outputs": [],
   "source": [
    "selected = 0\n",
    "data.set_index('sampling', drop=False, inplace=True)\n",
    "pdata.set_index('sampling', drop=False, inplace=True)\n",
    "for tp_i, tp in enumerate(to_process):\n",
    "    select = np.logical_and(\n",
    "        data[cname] == tp.cname, data[cunit] == tp.unit\n",
    "    )\n",
    "    selected += sum(select)\n",
    "    print(\n",
    "        \"%d Selected %d %s %s (%d)\"\n",
    "        % (\n",
    "            tp_i, sum(select),\n",
    "            tp.cname,\n",
    "            tp.unit,\n",
    "            len(data['sampling'][select].unique()),\n",
    "        )\n",
    "    )\n",
    "    mean = data.loc[select, :]\n",
    "    mean = mean.groupby(level=0).mean()\n",
    "    pdata = pdata.join(mean.loc[:, vname])\n",
    "    assert len(pdata) == pdata_len, (pdata_len, len(pdata))\n",
    "    if tp.trans is not None:\n",
    "        pdata[vname] = pdata[vname].map(tp.trans)\n",
    "\n",
    "    def missing(x, f):\n",
    "        if f not in x.columns:\n",
    "            return np.ones(len(x)).astype(bool)\n",
    "        x = x[f].astype(str)\n",
    "        ans = np.logical_or(np.equal(x, 'None'), np.equal(x, ''))\n",
    "        ans = np.logical_or(ans, np.equal(x, 'nan'))\n",
    "        ans = np.logical_or(ans, np.equal(x, 'NaN'))\n",
    "        return ans\n",
    "\n",
    "    missing0 = missing(pdata, tp.to)\n",
    "    if tp.to not in pdata.columns:\n",
    "        pdata.rename(columns={vname: tp.to}, inplace=True)\n",
    "    else:\n",
    "        # print(len(missing), sum(missing), missing.shape)\n",
    "        #X pdata[tp.to][missing0] = pdata[vname][missing0]\n",
    "        pdata.loc[missing0, tp.to] = pdata[vname][missing0]\n",
    "        pdata.drop(columns=vname, inplace=True)\n",
    "    missing1 = missing(pdata, tp.to)\n",
    "    print(\n",
    "        \"%d, Needed %d, used %d, still need %d\"\n",
    "        % (tp_i, sum(missing0), sum(missing0) - sum(missing1), sum(missing1))\n",
    "    )\n",
    "    assert len(pdata) == pdata_len, (pdata_len, len(pdata))\n",
    "print(\"Selected %s total\" % selected)\n",
    "pdata = pdata[~np.isnan(pdata['conc'])]\n",
    "# pdata = pdata[~np.isnan(pdata['flow'])]\n",
    "print(\"Lost %d to blank data\" % (pdata_len-len(pdata)))\n",
    "pdata.to_csv(\"pdata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TJs7_I32Arpu"
   },
   "source": [
    "Of 22,000 sampling events sampling either flow or P, we can't find flow *and* P for 4700.  Most (~3600) of the missing data is flow, so we *could* fill in with flow data, not doing that currently.\n",
    "\n",
    "Update: not dropping obs. for missing flow data now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I_kz258y4_mY"
   },
   "outputs": [],
   "source": [
    "# update locs with the number of obs. for each site\n",
    "count = pdata.groupby(by=['site']).count()\n",
    "count.rename(columns={count.columns[0]:'count'}, inplace=True)\n",
    "locs = merge(locs, count, ['site'], ['count'])\n",
    "\n",
    "# as above for min/max date\n",
    "count = pdata.groupby(by=['site']).min()\n",
    "locs = merge(locs, count, ['site'], ['ActivityStartDate'])\n",
    "locs.rename(columns={'ActivityStartDate': 'StartDate'}, inplace=True)\n",
    "count = pdata.groupby(by=['site']).max()\n",
    "# yes, use ActivityStartDate both times\n",
    "locs = merge(locs, count, ['site'], ['ActivityStartDate'])\n",
    "locs.rename(columns={'ActivityStartDate': 'EndDate'}, inplace=True)\n",
    "\n",
    "locs.to_csv(\"locs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FNVpWpYugpiW"
   },
   "source": [
    "# Link to flow data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yuBkoVLryzqb"
   },
   "source": [
    "Now we need flow data for places without conc. data.  Doesn't seem to be in the WQ portal, not strictly speaking a WQ parameter (well, it's Water Quantity, not Water Quality).\n",
    "\n",
    "Ran //waterservices.usgs.gov/nwis/site/?format=rdb&bBox=-80.040200,43.064600,-75.969700,44.355000&seriesCatalogOutput=true&siteType=ST&siteStatus=all and created `waterservices.usgs.gov.txt`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "1ivmBHjHzvOU",
    "outputId": "b084a94a-3184-411a-aa32-fb7a87a01f48"
   },
   "outputs": [],
   "source": [
    "floc = pd.read_csv(\n",
    "    \"waterservices.usgs.gov.txt\", sep=r'\\t', comment='#', engine='python', \n",
    "    skiprows=[43],  # the FORTRANesque column widths\n",
    "    dtype={'site_no': str}\n",
    ")\n",
    "print(\"Total\", len(floc))\n",
    "floc.drop_duplicates(subset=['dec_lat_va', 'dec_long_va', 'agency_cd', 'site_no'], inplace=True)\n",
    "print(\"Unique\", len(floc))\n",
    "floc = floc[~np.isnan(floc.dec_lat_va)]\n",
    "floc = floc[~np.isnan(floc.dec_long_va)]\n",
    "print(\"Non-null\", len(floc))\n",
    "floc.to_csv(\"locs2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SAODZpBXvbOK"
   },
   "source": [
    "The P sampling `locs` have their original lat/lon values, no necessarily consistent within a site.  That's ok, use these for intersect as it will give all possible answers for each site, we can resolve mulitples later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4UAswFxzwcHd",
    "outputId": "5ec0a224-25b5-4d28-f85a-9d09327bc12f"
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "mdnmap = folium.Map(location=[43.234853, -77.533949], zoom_start=19)\n",
    "\n",
    "for row in locs.itertuples():\n",
    "    folium.Marker([row.LatitudeMeasure, row.LongitudeMeasure], popup=row.site).add_to(mdnmap)\n",
    "mdnmap\n",
    "\n",
    "# ---\n",
    "locblobs = geopandas.GeoDataFrame(\n",
    "    locs,\n",
    "    geometry=geopandas.points_from_xy(\n",
    "        locs.LongitudeMeasure, locs.LatitudeMeasure\n",
    "    ),\n",
    "    crs = {'init' :'epsg:4326'},\n",
    ")\n",
    "locblobs.geometry = locblobs.geometry.to_crs(5071).buffer(50)\n",
    "\n",
    "# don't do this because we might want to use an up/down stream guage for\n",
    "# an unguaged location with lots of P obs.\n",
    "# although this doesn't entirely make sense as we're filtering on the presence\n",
    "# of at least 1 WQ sample of some kind...\n",
    "#\n",
    "# print(\"From\", len(locblobs), \"locations\")\n",
    "# locblobs = locblobs[~np.isnan(locblobs['count'])]\n",
    "# print(\"Go to\", len(locblobs), \"with P obs.\")\n",
    "\n",
    "flocblobs = geopandas.GeoDataFrame(\n",
    "    floc,\n",
    "    geometry=geopandas.points_from_xy(\n",
    "        floc.dec_long_va, floc.dec_lat_va\n",
    "    ),\n",
    "    crs = {'init' :'epsg:4326'},\n",
    ")\n",
    "flocblobs.geometry = flocblobs.geometry.to_crs(5071).buffer(50)\n",
    "# flocblobs['site_id'] = flocblobs.agency_cd + '-' + flocblobs.site_no\n",
    "# because geopandas.overlay seems to change site_no to float\n",
    "\n",
    "intersect = geopandas.overlay(locblobs, flocblobs, how='identity')\n",
    "\n",
    "floc.site_no.dtype\n",
    "flocblobs.site_no.dtype\n",
    "intersect.site_no.dtype\n",
    "# intersect.site_id.dtype\n",
    "\n",
    "\n",
    "lint = len(intersect)\n",
    "intersect = intersect[intersect.site_no.astype(str) != 'nan']\n",
    "print(\"Dropped %d NaN intersections\" % (lint-len(intersect)))\n",
    "\n",
    "# make a dict of site: (set of intersecting flow data)\n",
    "sets = {}\n",
    "for inter in intersect.itertuples():\n",
    "    site_1 = str(inter.site)\n",
    "    site_2 = \"%s-%s\" % (inter.agency_cd, inter.site_no)\n",
    "    sets.setdefault(site_1, set()).add(site_2)\n",
    "\n",
    "[{k:v} for k,v in sets.items() if len(v) > 1]\n",
    "toget = set()\n",
    "for flows in sets.values():\n",
    "    toget.update(flows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4UAswFxzwcHd",
    "outputId": "5ec0a224-25b5-4d28-f85a-9d09327bc12f"
   },
   "outputs": [],
   "source": [
    "tsv_tmpl = {\n",
    "    \"site_no\": 431510077363501,\n",
    "    # \"agency_cd\": \"USGS\",\n",
    "    \"inventory_output\": \"0\",\n",
    "    \"rdb_inventory_output\": \"file\",\n",
    "    \"begin_date\": \"2000-01-01\",\n",
    "    \"end_date\": \"2019-07-11\",\n",
    "    \"TZoutput\": \"0\",\n",
    "    \"pm_cd_compare\": \"Greater than\",\n",
    "    \"radio_parm_cds\": \"all_parm_cds\",\n",
    "    \"qw_attributes\": \"0\",\n",
    "    \"format\": \"rdb\",\n",
    "    \"qw_sample_wide\": \"wide\",\n",
    "    \"rdb_qw_attributes\": \"0\",\n",
    "    \"date_format\": \"YYYY-MM-DD\",\n",
    "    \"rdb_compression\": \"gz\",\n",
    "    \"submitted_form\": \"brief_list\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uploaded get_wqdata.py to Colab Notebooks folder, now add to import path\n",
    "import sys\n",
    "if BASE not in sys.path:\n",
    "    sys.path.append(str(BASE))\n",
    "from get_wqdata import GetWQData, WQ_API_URL\n",
    "\n",
    "import get_wqdata\n",
    "from importlib import reload\n",
    "reload(get_wqdata)\n",
    "GetWQData = get_wqdata.GetWQData\n",
    "WQ_API_URL = get_wqdata.WQ_API_URL\n",
    "\n",
    "wq = GetWQData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4UAswFxzwcHd",
    "outputId": "5ec0a224-25b5-4d28-f85a-9d09327bc12f"
   },
   "outputs": [],
   "source": [
    "flows = {}    \n",
    "params = 'p00060', 'p00061'\n",
    "for flow in toget:\n",
    "    flow = flow.replace(\"USGS-\", \"\")\n",
    "    # if flow != \"04249000\":\n",
    "    #     continue\n",
    "    query = dict(\n",
    "        format=\"json\",\n",
    "        indent=\"off\",\n",
    "        sites=flow,\n",
    "        startDT=\"2000-01-01\",\n",
    "        endDT=\"2019-07-10\",\n",
    "        parameterCd=\"00060\",  # ft3/s\n",
    "    )\n",
    "    jsonfile = wq.get_data(query, 'flow')\n",
    "    # print(WQ_API_URL['flow']+'?'+'&'.join(\"%s=%s\" % (k, v) for k, v in query.items() if not k.startswith('_')))\n",
    "    data = json.load(open(jsonfile))\n",
    "    data = data['value']['timeSeries']\n",
    "    if data:\n",
    "        data = data[0]['values']\n",
    "        assert len(data) == 1, len(data)\n",
    "        data = data[0]['value']\n",
    "        # print(flow, len(data))\n",
    "        flows[flow] = [(i['dateTime'], float(i['value'])) for i in data]\n",
    "        print(\"%s -> %s\" % (flow, jsonfile))\n",
    "    else:\n",
    "        # fall back to TSV data\n",
    "        query = tsv_tmpl.copy()\n",
    "        query['site_no'] = str(flow)\n",
    "        try:\n",
    "            datapath = wq.get_data(query, 'flowTSV')\n",
    "            print(datapath)\n",
    "            data = pd.read_csv(datapath)     \n",
    "            print(\"%s -> %s\" % (flow, datapath))\n",
    "            for param in params:\n",
    "                if param not in data.columns:\n",
    "                    print(\"No\", param)\n",
    "                    continue\n",
    "                flows[flow] = [(row['sample_dt'], row[param]) for row_i, row in data.iterrows()]\n",
    "        except OSError:  # not a gzip file\n",
    "            print(\"%s -> %s\" % (flow, 'NO DATA'))\n",
    "            continue\n",
    "\n",
    "# flows['431510077363501']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "_jMbPGCekgzA",
    "outputId": "f906c1a2-1b1b-4a24-9b7e-26b4a2de5ec6"
   },
   "outputs": [],
   "source": [
    "\n",
    "locs['sFlow'] = None\n",
    "locs['eFlow'] = None\n",
    "locs['cFlow'] = None\n",
    "for loc in locs.itertuples():\n",
    "    if not loc.count:\n",
    "        continue\n",
    "    if loc.site not in sets:\n",
    "        print(loc.site, \"no flow data source found\")\n",
    "        continue\n",
    "    flow = loc.site.replace(\"USGS-\", \"\")\n",
    "    if flow not in flows:\n",
    "        print(loc.site, \"no flow data found\")\n",
    "        continue\n",
    "    print(\"Populating\", loc.site)\n",
    "    for src in sets[loc.site]:\n",
    "        if loc.sFlow:\n",
    "            print(\"Overwriting data for\", loc.site)\n",
    "        locs.loc[loc.Index, 'sFlow'] = min(i[0] for i in flows[flow])\n",
    "        locs.loc[loc.Index, 'eFlow'] = max(i[0] for i in flows[flow])\n",
    "        locs.loc[loc.Index, 'cFlow'] = len(flows[flow])\n",
    "        # print(len(flows[flow]), 'for', locs.site)\n",
    "locs.to_csv(\"locs.csv\")\n",
    "sets['USGS-431510077363501']\n",
    "# flows['431510077363501']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nqgHc1-4yaYz"
   },
   "source": [
    "## Generate LOADEST inputs\n",
    "\n",
    "Needs two .csv files, `date, time, flow` for \"estimation\", and `date, time, conc, flow` for \"calibration\".\n",
    "\n",
    "Pull in flows from flow data then interpolate flow data with interpolation distance (in days).  Interpolate three sources of flow data, the water quality portal data in `pdata`, the daily values in flows, and the TSV data, also in flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_end_date(s, e):\n",
    "    \"\"\"Calculate midpoint time from start and end\"\"\"\n",
    "    s = parse(s)\n",
    "    try:\n",
    "        e = parse(e)\n",
    "    except AttributeError:  # when parse() gets a NaN from pandas\n",
    "        e = s\n",
    "    return s + (e - s) / 2  # midpoint from adding half timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdata['date'] = [\n",
    "    start_end_date(i.ActivityStartDate, i.ActivityEndDate).date()\n",
    "    for i in pdata.itertuples()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, collect actual flow observations by site\n",
    "flow_itrp = {}\n",
    "for row in locs.itertuples():\n",
    "    site = row.site\n",
    "    flow_obs = []\n",
    "    for flow in pdata[pdata.site == site].itertuples():\n",
    "        if np.isnan(flow.flow):\n",
    "            continue\n",
    "        s = flow.date\n",
    "        flow_obs.append((s, flow.flow, 'pflow'))\n",
    "    for flow in flows.get(site.replace(\"USGS-\", \"\"), []):\n",
    "        try:\n",
    "            f = float(flow[1])\n",
    "        except ValueError:\n",
    "            continue  # \"E 1.0\" etc.\n",
    "        if np.isnan(f):\n",
    "            continue\n",
    "        src = 'daily' if 'T' in flow[0] else 'TSV'\n",
    "        flow_obs.append((parse(flow[0]).date(), f, src))\n",
    "    if flow_obs:\n",
    "        flow_obs.sort()\n",
    "        flow_itrp[site] = flow_obs\n",
    "# [i for i in flow_itrp['USGS-04232050'] if i[0].year == 2016 and i[0].month == 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then interpolate these actual observation lists\n",
    "max_interp = 10  # days\n",
    "for site in flow_itrp:\n",
    "    flow_obs = flow_itrp[site]\n",
    "    res = []\n",
    "    i = 0\n",
    "    while i < len(flow_obs) - 1:\n",
    "        tsep = (flow_obs[i+1][0] - flow_obs[i][0]).days\n",
    "        date = flow_obs[i][0]\n",
    "        if tsep > max_interp:\n",
    "            i += 1\n",
    "            res.append((date, flow_obs[i][1], 0))\n",
    "            continue\n",
    "        vsep = flow_obs[i+1][1] - flow_obs[i][1]\n",
    "        while date < flow_obs[i+1][0]:\n",
    "            prop = (date - flow_obs[i][0]).days / tsep\n",
    "            itrp = flow_obs[i][1] + prop * vsep\n",
    "            dist = min(prop*tsep, (1-prop)*tsep)\n",
    "            res.append((date, itrp, dist))\n",
    "            date += timedelta(days=1)\n",
    "        i += 1\n",
    "        while i < len(flow_obs) - 2 and date == flow_obs[i+1][0]:\n",
    "            i += 1\n",
    "    flow_itrp[site] = res\n",
    "\n",
    "    outpath = Path(\"loadests\").joinpath(site)\n",
    "    outpath.mkdir(parents=True, exist_ok=True)\n",
    "    outfile = outpath.joinpath(\"%s_est.csv\" % site)\n",
    "    res = pd.DataFrame(data=dict(\n",
    "        date=[i[0].strftime(\"%Y%m%d\") for i in res],\n",
    "        time='12:00',\n",
    "        flow=[i[1] for i in res],\n",
    "        itrp=[i[2] for i in res],\n",
    "    ))\n",
    "    res.to_csv(outfile, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sites where interpolation occured\n",
    "' '.join([k for k, v in flow_itrp.items() if any(i[2] > 0 for i in v)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(flow_itrp)\n",
    "flow_itrp = {k:v for k, v in flow_itrp.items() if len(v) >= 20}\n",
    "print(\"Dropped %d sites < 20 flow obs.\" % (n-len(flow_itrp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddropped = 0\n",
    "fdropped = 0\n",
    "\n",
    "for site in flow_itrp:\n",
    "    if False and site != 'USGS-04232050':\n",
    "        continue\n",
    "        \n",
    "    calobs = []\n",
    "    sitep = pdata[pdata.site == site].copy()\n",
    "    # sitep['date'] = [\n",
    "    #     start_end_date(i.ActivityStartDate, i.ActivityEndDate)\n",
    "    #     for i in sitep.itertuples()\n",
    "    # ]\n",
    "    n = len(sitep)\n",
    "    sitep.drop_duplicates(subset=['date'], inplace=True)\n",
    "    n -= len(sitep)\n",
    "    if n:\n",
    "        print(\"Dropped %s %d obs. on repeated dates\" % (site, n))\n",
    "        ddropped += n\n",
    "    iflow = {i[0]:i[1] for i in flow_itrp.get(site, [])}\n",
    "    sitep['iflow'] = [iflow.get(i, np.NaN) for i in sitep.date]\n",
    "    # use general flow only in absence of specific flow from WQ data\n",
    "    missing = np.isnan(sitep.flow)\n",
    "    sitep.loc[missing, 'flow'] = sitep.loc[missing, 'iflow']\n",
    "    n = len(sitep)\n",
    "    sitep = sitep[~np.isnan(sitep.flow)]\n",
    "    n -= len(sitep)\n",
    "    if n:\n",
    "        print(\"Dropped %s %d obs. for no flow data\" % (site, n))\n",
    "        fdropped += n\n",
    "    outpath = Path(\"loadests\").joinpath(site)\n",
    "    outpath.mkdir(parents=True, exist_ok=True)\n",
    "    outfile = outpath.joinpath(\"%s_calib.csv\" % site)\n",
    "\n",
    "    estfile = outpath.joinpath(\"%s_est.csv\" % site)\n",
    "    pltfile = outpath.joinpath(\"%s_data.pdf\" % site)\n",
    "    sitef = pd.read_csv(estfile)\n",
    "    plt.clf()\n",
    "    plt.gcf().set_size_inches(20, 6)\n",
    "    # this just runs without returning\n",
    "    # markerline, stemlines, baseline = plt.stem(sitep.date, sitep.flow)\n",
    "    # plt.setp(stemlines, 'linewidth', 0.3)\n",
    "    # markerline, stemlines, baseline = plt.stem([parse(str(i)) for i in sitef.date], sitef.flow)\n",
    "    # plt.setp(stemlines, 'linewidth', 0.3)\n",
    "    plt.plot([parse(str(i)) for i in sitef.date], sitef.flow, lw=0.3)  #, s=0.8)\n",
    "    plt.scatter(sitep.date, sitep.flow, lw=0.3, s=0.4)\n",
    "    ax2 = plt.gca().twinx()\n",
    "    ax2.scatter(sitep.date, sitep.conc, s=0.5, c='k')\n",
    "    plt.savefig(pltfile)\n",
    "\n",
    "    sitep = pd.DataFrame(data=dict(\n",
    "        date=[i.strftime(\"%Y%m%d\") for i in sitep.date],\n",
    "        time='12:00',\n",
    "        flow=sitep.iflow,\n",
    "        conc=sitep.conc,\n",
    "    ))\n",
    "    sitep.to_csv(outfile, index=False)\n",
    "        \n",
    "print(\"Dropped %d obs. on repeated dates\" % ddropped)\n",
    "print(\"Dropped %d P obs. with no flow\" % fdropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in flow_itrp['USGS-04232050'] if i[0].year == 2016 and i[0].month == 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in flows['04232050'] if '2016-06-13' in i[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse('2016-06-13T00:00:00.000')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "LOads.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
