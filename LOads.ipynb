{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LOads.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tbnorth/LOads_colab/blob/master/LOads.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NW-Jh5er5kCq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62UIOjgh6s1C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\")\n",
        "ROOT = Path(\"/content/gdrive/My Drive/Colab Notebooks\")\n",
        "BASE = ROOT.joinpath('LOads')\n",
        "BASE.mkdir(exist_ok=True)\n",
        "os.chdir(BASE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyaGn8iZEi3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import namedtuple\n",
        "from dateutil.parser import parse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wheVzmbh73ny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# uploaded get_wqdata.py to Colab Notebooks folder, now add to import path\n",
        "import sys\n",
        "if BASE not in sys.path:\n",
        "    sys.path.append(str(BASE))\n",
        "from get_wqdata import GetWQData\n",
        "\n",
        "wq = GetWQData()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cx15z3nOB5uJ",
        "colab_type": "text"
      },
      "source": [
        "Want to work out which CharacteristicName values to request.  The WQ portal web interface says they come [from here](http://iaspub.epa.gov/sor_internet/registry/substreg/home/overview/home.do) but that's hard to search, so request all obs. in our bounding box and search that list instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOM5hZUYCYxe",
        "colab_type": "code",
        "collapsed": true,
        "colab": {}
      },
      "source": [
        "query = dict(\n",
        "    _desc=\"All sample types\",\n",
        "    bBox=\"-80.0402,43.0646,-75.9697,44.355\",\n",
        "    siteType=[\"Aggregate surface-water-use\", \"Stream\"],\n",
        "    sampleMedia=[\"Water\", \"water\"],\n",
        "    # siteid=[\"USGS-04231600\"],\n",
        "    # startDateLo=\"01-01-2018\",\n",
        "    # startDateHi=\"02-01-2018\",\n",
        ")\n",
        "print(\"Reading data\")\n",
        "all_data = wq.get_data(query)  # gets path to .csv file\n",
        "d = pd.read_csv(all_data)  # , nrows=10)\n",
        "print(\"Data read\")\n",
        "print(d.columns)\n",
        "po4 = set(d[\"CharacteristicName\"])\n",
        "keep = set()\n",
        "for term in 'po4', 'phosphor', 'srp', 'phosphate', 'flow', 'guage', 'cfs':\n",
        "    for cname in po4:\n",
        "        if term in cname.lower():\n",
        "            keep.add(cname)\n",
        "print(sorted(keep))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZxLuVimEBVF",
        "colab_type": "text"
      },
      "source": [
        "Now make a sensible list from the values we found."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVO8fq7dBn55",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_cache = {\n",
        "    'characteric_names': [\n",
        "        # 'O-Ethyl O-methyl S-propyl phosphorothioate',\n",
        "        # 'O-Ethyl S-methyl S-propyl phosphorodithioate',\n",
        "        # 'O-Ethyl S-propyl phosphorothioate',\n",
        "        # 'Organic phosphorus',\n",
        "        'Orthophosphate',\n",
        "        'Orthophosphate as P',\n",
        "        'Phosphate-phosphorus',\n",
        "        'Phosphate-phosphorus as P',\n",
        "        'Phosphorus',\n",
        "        'Stream flow, instantaneous',\n",
        "        'Stream flow, mean. daily',\n",
        "        # 'Tributyl phosphate',\n",
        "        # 'Triphenyl phosphate',\n",
        "        # 'Tris(1,3-dichloro-2-propyl)phosphate',\n",
        "        # 'Tris(2-butoxyethyl) phosphate',\n",
        "        # 'Tris(2-chloroethyl) phosphate',\n",
        "    ]\n",
        "}\n",
        "\n",
        "names = use_cache.get('characteric_names')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXAT9ESNFZXc",
        "colab_type": "text"
      },
      "source": [
        "Now grab all the data we're interested in"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLzhWPqJFQ7s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query = dict(\n",
        "    _desc=\"Target sample types\",\n",
        "    bBox=\"-80.0402,43.0646,-75.9697,44.355\",\n",
        "    siteType=[\"Aggregate surface-water-use\", \"Stream\"],\n",
        "    sampleMedia=[\"Water\", \"water\"],\n",
        "    characteristicName=names,\n",
        ")\n",
        "print(\"Reading data\")\n",
        "# first get the Site info. flavored response\n",
        "data = wq.get_data(query, type_='site')  # gets path to .csv file\n",
        "site = pd.read_csv(data)\n",
        "# print('\\n'.join(sorted(site.columns)))\n",
        "# then get the Result info.\n",
        "data = wq.get_data(query, type_='result')\n",
        "d = pd.read_csv(data, low_memory=False)\n",
        "d.columns = [i.replace('/', '_') for i in d.columns]\n",
        "# get rid of \"mg/l<space><space><space>\" units\n",
        "for col in ['ResultMeasure_MeasureUnitCode']:\n",
        "    d[col] = [str(i).strip() for i in d[col]]\n",
        "had = len(d)\n",
        "d = d.loc[d['ResultMeasure_MeasureUnitCode'] != 'nan', :]\n",
        "print(\"Lost %d for missing units\" % (had - len(d)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqUj6aVFGVV2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# find common columns\n",
        "common = set(site.columns).intersection(set(d.columns))\n",
        "print('\\n'.join(sorted(common)))\n",
        "# use this common column\n",
        "common = 'MonitoringLocationIdentifier'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlnI67nVGmFe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a list of unique sites\n",
        "locs = site.loc[:, [common, 'LatitudeMeasure', 'LongitudeMeasure']]\n",
        "locs.drop_duplicates(inplace=True)\n",
        "length = len(locs)\n",
        "length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePo6lw_0HCh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# copy lat / lon to main data\n",
        "old_len = len(d)\n",
        "# d = d.join(locs, rsuffix='CULL')\n",
        "d = d.join(locs.loc[:, ['LatitudeMeasure', 'LongitudeMeasure']])\n",
        "assert len(d) == old_len\n",
        "# d.drop(columns=[i for i in d.columns if 'CULL' in i], inplace=True)\n",
        "d.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7n1oI1XI178",
        "colab_type": "text"
      },
      "source": [
        "Data has different `MonitoringLocationIdentifier`s for the same coords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkFwnJS9IDWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate a location based site field\n",
        "dd = d.copy(deep=False)\n",
        "# round lat lon to snap nearby points together\n",
        "dd['_lat'] = np.round(dd['LatitudeMeasure'], 3)\n",
        "dd['_lon'] = np.round(dd['LongitudeMeasure'], 3)\n",
        "dd.set_index(['_lat', '_lon'], inplace=True)\n",
        "# use mean of snapped points, same as original for\n",
        "# points that were all together to start with\n",
        "mean = dd.groupby(by=['_lat', '_lon']).mean()\n",
        "dd = dd.join(mean, on=['_lat', '_lon'], rsuffix=\"MEAN\")\n",
        "d['lat'] = dd['LatitudeMeasureMEAN'].values\n",
        "d['lon'] = dd['LongitudeMeasureMEAN'].values\n",
        "d['site'] = np.round(d['lat'],4).astype(str) + np.round(d['lon'], 4).astype(str)\n",
        "assert len(d) == old_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pXzMa7wJ-3H",
        "colab_type": "text"
      },
      "source": [
        "Now copy site info. into locs table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgY12QBVKEtF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d.set_index(['LatitudeMeasure', 'LongitudeMeasure'], inplace=True, drop=False)\n",
        "locs = locs.join(d, on=['LatitudeMeasure', 'LongitudeMeasure'], rsuffix='CULL')\n",
        "for col in [i for i in locs.columns if 'CULL' in i]:\n",
        "    locs.drop(columns=col, inplace=True)\n",
        "locs.drop_duplicates(inplace=True, subset=['site', 'MonitoringLocationIdentifier'])\n",
        "locs.to_csv(\"locs.csv\", index=False)\n",
        "offset = max((locs['lat']-locs['LatitudeMeasure']).abs())\n",
        "assert offset < 0.001, offset\n",
        "assert len(locs) == length, (length, len(locs))\n",
        "locs.to_csv(\"locs.csv\")\n",
        "offset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1stEddL4jqzb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"%d locations\" % len(locs))\n",
        "distinct = locs.set_index([\"LatitudeMeasure\", \"LongitudeMeasure\"]).index\n",
        "print(\"%d distinct\" % len(distinct.unique()))\n",
        "distinct = locs.set_index([\"lat\", \"lon\"]).index\n",
        "print(\"%d really distinct\" % len(distinct.unique()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVH5IJXWkhUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tabulate units used for results\n",
        "d['ResultSampleFractionText'] = [\n",
        "    i if i != 'nan' else 'NA'\n",
        "    for i in d['ResultSampleFractionText'].astype(str)\n",
        "]\n",
        "\n",
        "res_types = (\n",
        "    d.groupby(\n",
        "        [\n",
        "            'CharacteristicName',\n",
        "            'ResultMeasure_MeasureUnitCode',\n",
        "            'ResultSampleFractionText',\n",
        "        ]\n",
        "    )\n",
        "    .count()\n",
        "    .iloc[:, 0]\n",
        ")\n",
        "print(res_types)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvDaZ5-pkvVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "duration = wq.db.get('duration', [])  # cache expensive calc. in wq.db\n",
        "if not duration:\n",
        "    for row in d.itertuples():\n",
        "        try:\n",
        "            t0 = parse(\n",
        "                \"%s %s\"\n",
        "                % (row.ActivityStartDate, row.ActivityStartTime_Time)\n",
        "            )\n",
        "            t1 = parse(\n",
        "                \"%s %s\" % (row.ActivityEndDate, row.ActivityEndTime_Time)\n",
        "            )\n",
        "            duration.append((t1 - t0).seconds)\n",
        "        except ValueError:\n",
        "            duration.append(0)\n",
        "    wq.db['duration'] = duration\n",
        "d['duration'] = duration"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIbZBmsClx1c",
        "colab_type": "text"
      },
      "source": [
        "Create an ID field for each sampling event"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYZXRm5jlp-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d['sampling'] = (\n",
        "    d['ActivityStartTime_Time'].astype(str)\n",
        "    + d['ActivityEndTime_Time'].astype(str)\n",
        "    + ' '\n",
        "    + d['ActivityStartDate'].astype(str)\n",
        "    + ' '\n",
        "    + d['ActivityEndDate'].astype(str)\n",
        "    + ' '\n",
        "    + d['LatitudeMeasure'].astype(str)\n",
        "    + ' '\n",
        "    + d['LongitudeMeasure'].astype(str)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Wv941RAmIeb",
        "colab_type": "text"
      },
      "source": [
        "## Save output data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLjS3EC3mMfL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d.to_csv('data.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PqLby2umSla",
        "colab_type": "text"
      },
      "source": [
        "Now we want to match P conc. data and flow data for each sampling event\n",
        "\n",
        "(start to refer to `d` as `data` from here on)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHhBDk2nmh0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(\"data.csv\", low_memory=False)\n",
        "pdata = data.loc[:, ['sampling', 'site']]\n",
        "pdata.drop_duplicates(subset=['sampling'], inplace=True)\n",
        "pdata_len = len(pdata)\n",
        "print(\"%d into %d\" % (len(data), pdata_len))\n",
        "cname = 'CharacteristicName'\n",
        "cunit = 'ResultMeasure_MeasureUnitCode'\n",
        "vname = 'ResultMeasureValue'\n",
        "TP = namedtuple(\"ToProcess\", \"to cname unit trans\")\n",
        "po4_to_p = (30.97 + 4 * 16) / 30.97\n",
        "to_process = [\n",
        "    TP('flow', 'Stream flow, instantaneous', 'ft3/s', None),\n",
        "    TP(\n",
        "        'flow',\n",
        "        'Stream flow, instantaneous',\n",
        "        'm3/sec',\n",
        "        lambda x: x * 35.3147,\n",
        "    ),\n",
        "    TP('flow', 'Stream flow, mean. daily', 'ft3/s', None),\n",
        "    TP('conc', 'Phosphorus', 'mg/l as P', None),\n",
        "    TP('conc', 'Phosphorus', 'mg/l', None),\n",
        "    TP('conc', 'Phosphorus', 'ug/l', lambda x: 1000 * x),\n",
        "    TP('conc', 'Phosphorus', 'mg/l PO4', lambda x: x / po4_to_p),\n",
        "    TP('conc', 'Phosphate-phosphorus as P', 'mg/l', None),\n",
        "    TP('conc', 'Phosphate-phosphorus', 'mg/l', None),\n",
        "    TP('conc', 'Orthophosphate as P', 'mg/l', None),\n",
        "    TP('conc', 'Orthophosphate', 'mg/l as P', None),\n",
        "    TP('conc', 'Orthophosphate', 'mg/l asPO4', lambda x: x / po4_to_p),\n",
        "    TP('conc', 'Orthophosphate', 'mg/l', lambda x: x / po4_to_p),\n",
        "    # exclude as it's in bed sediment\n",
        "    # TP('conc', 'Phosphorus', 'mg/kg as P', None),\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPGAFGYinYDL",
        "colab_type": "text"
      },
      "source": [
        "We're merging 82386 observations into 21877 flow + conc. records.  In a perfect world we'd have one flow and one conc. record for each sampling, but we have both redundant and mismatched (flow without conc. and visa versa) records, roughly 4:1 instead of 2:1.\n",
        "\n",
        "`to_process` is an **ordered** list of items where each item reocords the field we're trying to fill (`to`), the `CharacteristicName` (`cname`) of our prefered source field, our prefered `unit`, and any conversion (`trans`) necessary to use this source field / unit combination.\n",
        "\n",
        "Now we're going to fill the `to` fields (`flow`, `conc`) with the values selected using the list items, filling as many records with the first set of selected data, and only filling missing records with the next (less desireable) list item."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eb6WyR0ozty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "selected = 0\n",
        "data.set_index('sampling', drop=False, inplace=True)\n",
        "pdata.set_index('sampling', drop=False, inplace=True)\n",
        "for tp_i, tp in enumerate(to_process):\n",
        "    select = np.logical_and(\n",
        "        data[cname] == tp.cname, data[cunit] == tp.unit\n",
        "    )\n",
        "    selected += sum(select)\n",
        "    print(\n",
        "        \"%d Selected %d %s %s (%d)\"\n",
        "        % (\n",
        "            tp_i, sum(select),\n",
        "            tp.cname,\n",
        "            tp.unit,\n",
        "            len(data['sampling'][select].unique()),\n",
        "        )\n",
        "    )\n",
        "    mean = data.loc[select, :]\n",
        "    mean = mean.groupby(level=0).mean()\n",
        "    pdata = pdata.join(mean.loc[:, vname])\n",
        "    assert len(pdata) == pdata_len, (pdata_len, len(pdata))\n",
        "    if tp.trans is not None:\n",
        "        pdata[vname] = pdata[vname].map(tp.trans)\n",
        "\n",
        "    def missing(x, f):\n",
        "        if f not in x.columns:\n",
        "            return np.ones(len(x)).astype(bool)\n",
        "        x = x[f].astype(str)\n",
        "        ans = np.logical_or(np.equal(x, 'None'), np.equal(x, ''))\n",
        "        ans = np.logical_or(ans, np.equal(x, 'nan'))\n",
        "        ans = np.logical_or(ans, np.equal(x, 'NaN'))\n",
        "        return ans\n",
        "\n",
        "    missing0 = missing(pdata, tp.to)\n",
        "    if tp.to not in pdata.columns:\n",
        "        pdata.rename(columns={vname: tp.to}, inplace=True)\n",
        "    else:\n",
        "        # print(len(missing), sum(missing), missing.shape)\n",
        "        #X pdata[tp.to][missing0] = pdata[vname][missing0]\n",
        "        pdata.loc[missing0, tp.to] = pdata[vname][missing0]\n",
        "        pdata.drop(columns=vname, inplace=True)\n",
        "    missing1 = missing(pdata, tp.to)\n",
        "    print(\n",
        "        \"%d, Needed %d, used %d, still need %d\"\n",
        "        % (tp_i, sum(missing0), sum(missing0) - sum(missing1), sum(missing1))\n",
        "    )\n",
        "    assert len(pdata) == pdata_len, (pdata_len, len(pdata))\n",
        "print(\"Selected %s total\" % selected)\n",
        "pdata = pdata[~np.isnan(pdata['conc'])]\n",
        "pdata = pdata[~np.isnan(pdata['flow'])]\n",
        "print(\"Lost %d to blank data\" % (pdata_len-len(pdata)))\n",
        "pdata.to_csv(\"pdata.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_kz258y4_mY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# update locs with the number of obs. for each site\n",
        "count = pdata.groupby(by=['site']).count().iloc[:, 0]\n",
        "count.columns = ['count']\n",
        "locs = locs.join(count, on='site')\n",
        "locs.to_csv(\"locs.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuBkoVLryzqb",
        "colab_type": "text"
      },
      "source": [
        "Now we need flow data for places without conc. data.  Doesn't seem to be in the WQ portal, not strictly speaking a WQ parameter (well, it's Water Quantity, not Water Quality).\n",
        "\n",
        "Ran //waterservices.usgs.gov/nwis/site/?format=rdb&bBox=-80.040200,43.064600,-75.969700,44.355000&seriesCatalogOutput=true&siteType=ST&siteStatus=all and created `waterservices.usgs.gov.txt`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ivmBHjHzvOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d = pd.read_csv(\n",
        "    \"waterservices.usgs.gov.txt\", sep=r'\\t', comment='#', engine='python'\n",
        ")\n",
        "print(len(d))\n",
        "d.set_index(['dec_lat_va', 'dec_long_va'], inplace=True, drop=False)\n",
        "print(len(d.index.unique()))\n",
        "d.groupby(level=[0, 1]).min().to_csv(\"locs2.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}