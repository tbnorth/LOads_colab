{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LOads.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tbnorth/LOads_colab/blob/master/LOads.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONOS-z05f6dp",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW-Jh5er5kCq",
        "colab_type": "text"
      },
      "source": [
        "```python\n",
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62UIOjgh6s1C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f78c92e2-dd8c-4566-9bd6-3379c04e2b71"
      },
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\")\n",
        "ROOT = Path(\"/content/gdrive/My Drive/Colab Notebooks\")\n",
        "BASE = ROOT.joinpath('LOads')\n",
        "BASE.mkdir(exist_ok=True)\n",
        "os.chdir(BASE)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyaGn8iZEi3h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "7a46b43d-08a3-4700-8eb7-487483ae4bb1"
      },
      "source": [
        "!apt-get install libspatialindex-dev python3-rtree\n",
        "!pip install geopandas\n",
        "!ldconfig\n",
        "from rtree import index\n",
        "from rtree.index import Rtree\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas\n",
        "from collections import namedtuple\n",
        "from dateutil.parser import parse\n",
        "import json"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libspatialindex-dev is already the newest version (1.8.5-5).\n",
            "python3-rtree is already the newest version (0.8.3+ds-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 4 not upgraded.\n",
            "Requirement already satisfied: geopandas in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Requirement already satisfied: pyproj in /usr/local/lib/python3.6/dist-packages (from geopandas) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from geopandas) (0.24.2)\n",
            "Requirement already satisfied: fiona in /usr/local/lib/python3.6/dist-packages (from geopandas) (1.8.6)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.6/dist-packages (from geopandas) (1.6.4.post2)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->geopandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas->geopandas) (2.5.3)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from pandas->geopandas) (1.16.4)\n",
            "Requirement already satisfied: click-plugins>=1.0 in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas) (1.1.1)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas) (0.5.0)\n",
            "Requirement already satisfied: click<8,>=4.0 in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas) (7.0)\n",
            "Requirement already satisfied: attrs>=17 in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas) (19.1.0)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas) (2.3.2)\n",
            "Requirement already satisfied: six>=1.7 in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3E2_7srZVvJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def merge(left, right, on, cols):\n",
        "    for col in cols:\n",
        "        if col in left.columns:\n",
        "            left = left.drop(columns=col)\n",
        "    right = right.reset_index()\n",
        "    return left.merge(right[on+cols], on=on, how='left')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wheVzmbh73ny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# uploaded get_wqdata.py to Colab Notebooks folder, now add to import path\n",
        "import sys\n",
        "if BASE not in sys.path:\n",
        "    sys.path.append(str(BASE))\n",
        "from get_wqdata import GetWQData, WQ_API_URL\n",
        "\n",
        "wq = GetWQData()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Mm-pDqcgB0Y",
        "colab_type": "text"
      },
      "source": [
        "# Explore dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cx15z3nOB5uJ",
        "colab_type": "text"
      },
      "source": [
        "Want to work out which CharacteristicName values to request.  The WQ portal web interface says they come [from here](http://iaspub.epa.gov/sor_internet/registry/substreg/home/overview/home.do) but that's hard to search, so request all obs. in our bounding box and search that list instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOM5hZUYCYxe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "048e983f-1144-4350-8834-67966e870ca6"
      },
      "source": [
        "query = dict(\n",
        "    _desc=\"All sample types\",\n",
        "    bBox=\"-80.0402,43.0646,-75.9697,44.355\",\n",
        "    siteType=[\"Aggregate surface-water-use\", \"Stream\"],\n",
        "    sampleMedia=[\"Water\", \"water\"],\n",
        "    # siteid=[\"USGS-04231600\"],\n",
        "    # startDateLo=\"01-01-2018\",\n",
        "    # startDateHi=\"02-01-2018\",\n",
        ")\n",
        "print(\"Reading data\")\n",
        "all_data = wq.get_data(query)  # gets path to .csv file\n",
        "d = pd.read_csv(all_data)  # , nrows=10)\n",
        "print(\"Data read\")\n",
        "print(d.columns)\n",
        "po4 = set(d[\"CharacteristicName\"])\n",
        "keep = set()\n",
        "for term in 'po4', 'phosphor', 'srp', 'phosphate', 'flow', 'guage', 'cfs':\n",
        "    for cname in po4:\n",
        "        if term in cname.lower():\n",
        "            keep.add(cname)\n",
        "print(sorted(keep))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (9,10,11,13,19,20,22,24,25,33,35,39,41,42,44,55,56,57,61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Data read\n",
            "Index(['OrganizationIdentifier', 'OrganizationFormalName',\n",
            "       'ActivityIdentifier', 'ActivityTypeCode', 'ActivityMediaName',\n",
            "       'ActivityMediaSubdivisionName', 'ActivityStartDate',\n",
            "       'ActivityStartTime/Time', 'ActivityStartTime/TimeZoneCode',\n",
            "       'ActivityEndDate', 'ActivityEndTime/Time',\n",
            "       'ActivityEndTime/TimeZoneCode',\n",
            "       'ActivityDepthHeightMeasure/MeasureValue',\n",
            "       'ActivityDepthHeightMeasure/MeasureUnitCode',\n",
            "       'ActivityDepthAltitudeReferencePointText',\n",
            "       'ActivityTopDepthHeightMeasure/MeasureValue',\n",
            "       'ActivityTopDepthHeightMeasure/MeasureUnitCode',\n",
            "       'ActivityBottomDepthHeightMeasure/MeasureValue',\n",
            "       'ActivityBottomDepthHeightMeasure/MeasureUnitCode', 'ProjectIdentifier',\n",
            "       'ActivityConductingOrganizationText', 'MonitoringLocationIdentifier',\n",
            "       'ActivityCommentText', 'SampleAquifer', 'HydrologicCondition',\n",
            "       'HydrologicEvent', 'SampleCollectionMethod/MethodIdentifier',\n",
            "       'SampleCollectionMethod/MethodIdentifierContext',\n",
            "       'SampleCollectionMethod/MethodName', 'SampleCollectionEquipmentName',\n",
            "       'ResultDetectionConditionText', 'CharacteristicName',\n",
            "       'ResultSampleFractionText', 'ResultMeasureValue',\n",
            "       'ResultMeasure/MeasureUnitCode', 'MeasureQualifierCode',\n",
            "       'ResultStatusIdentifier', 'StatisticalBaseCode', 'ResultValueTypeName',\n",
            "       'ResultWeightBasisText', 'ResultTimeBasisText',\n",
            "       'ResultTemperatureBasisText', 'ResultParticleSizeBasisText',\n",
            "       'PrecisionValue', 'ResultCommentText', 'USGSPCode',\n",
            "       'ResultDepthHeightMeasure/MeasureValue',\n",
            "       'ResultDepthHeightMeasure/MeasureUnitCode',\n",
            "       'ResultDepthAltitudeReferencePointText', 'SubjectTaxonomicName',\n",
            "       'SampleTissueAnatomyName', 'ResultAnalyticalMethod/MethodIdentifier',\n",
            "       'ResultAnalyticalMethod/MethodIdentifierContext',\n",
            "       'ResultAnalyticalMethod/MethodName', 'MethodDescriptionText',\n",
            "       'LaboratoryName', 'AnalysisStartDate', 'ResultLaboratoryCommentText',\n",
            "       'DetectionQuantitationLimitTypeName',\n",
            "       'DetectionQuantitationLimitMeasure/MeasureValue',\n",
            "       'DetectionQuantitationLimitMeasure/MeasureUnitCode',\n",
            "       'PreparationStartDate', 'ProviderName'],\n",
            "      dtype='object')\n",
            "['O-Ethyl O-methyl S-propyl phosphorothioate', 'O-Ethyl S-methyl S-propyl phosphorodithioate', 'O-Ethyl S-propyl phosphorothioate', 'Organic phosphorus', 'Orthophosphate', 'Orthophosphate as P', 'Phosphate-phosphorus', 'Phosphate-phosphorus as P', 'Phosphorus', 'Stream flow, instantaneous', 'Stream flow, mean. daily', 'Tributyl phosphate', 'Triphenyl phosphate', 'Tris(1,3-dichloro-2-propyl)phosphate', 'Tris(2-butoxyethyl) phosphate', 'Tris(2-chloroethyl) phosphate']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZxLuVimEBVF",
        "colab_type": "text"
      },
      "source": [
        "Now make a sensible list from the values we found."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVO8fq7dBn55",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_cache = {\n",
        "    'characteric_names': [\n",
        "        # 'O-Ethyl O-methyl S-propyl phosphorothioate',\n",
        "        # 'O-Ethyl S-methyl S-propyl phosphorodithioate',\n",
        "        # 'O-Ethyl S-propyl phosphorothioate',\n",
        "        # 'Organic phosphorus',\n",
        "        'Orthophosphate',\n",
        "        'Orthophosphate as P',\n",
        "        'Phosphate-phosphorus',\n",
        "        'Phosphate-phosphorus as P',\n",
        "        'Phosphorus',\n",
        "        'Stream flow, instantaneous',\n",
        "        'Stream flow, mean. daily',\n",
        "        # 'Tributyl phosphate',\n",
        "        # 'Triphenyl phosphate',\n",
        "        # 'Tris(1,3-dichloro-2-propyl)phosphate',\n",
        "        # 'Tris(2-butoxyethyl) phosphate',\n",
        "        # 'Tris(2-chloroethyl) phosphate',\n",
        "    ]\n",
        "}\n",
        "\n",
        "names = use_cache.get('characteric_names')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXAT9ESNFZXc",
        "colab_type": "text"
      },
      "source": [
        "Now grab all the data we're interested in"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLzhWPqJFQ7s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ad659483-bd3c-4099-8f6d-551600b70c03"
      },
      "source": [
        "query = dict(\n",
        "    _desc=\"Target sample types\",\n",
        "    bBox=\"-80.0402,43.0646,-75.9697,44.355\",\n",
        "    siteType=[\"Aggregate surface-water-use\", \"Stream\"],\n",
        "    sampleMedia=[\"Water\", \"water\"],\n",
        "    characteristicName=names,\n",
        ")\n",
        "print(\"Reading data\")\n",
        "# first get the Site info. flavored response\n",
        "data = wq.get_data(query, type_='site')  # gets path to .csv file\n",
        "site = pd.read_csv(data)\n",
        "# print('\\n'.join(sorted(site.columns)))\n",
        "# then get the Result info.\n",
        "data = wq.get_data(query, type_='result')\n",
        "d = pd.read_csv(data, low_memory=False)\n",
        "d.columns = [i.replace('/', '_') for i in d.columns]\n",
        "# get rid of \"mg/l<space><space><space>\" units\n",
        "for col in ['ResultMeasure_MeasureUnitCode']:\n",
        "    d[col] = [str(i).strip() for i in d[col]]\n",
        "had = len(d)\n",
        "d = d.loc[d['ResultMeasure_MeasureUnitCode'] != 'nan', :]\n",
        "print(\"Lost %d for missing units\" % (had - len(d)))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading data\n",
            "Lost 2251 for missing units\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqUj6aVFGVV2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "fc6329ab-91b4-43d3-c3df-6fc7eb310c10"
      },
      "source": [
        "# find common columns\n",
        "common = set(site.columns).intersection(set(d.columns))\n",
        "print('\\n'.join(sorted(common)))\n",
        "# use this common column\n",
        "common = 'MonitoringLocationIdentifier'"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MonitoringLocationIdentifier\n",
            "OrganizationFormalName\n",
            "OrganizationIdentifier\n",
            "ProviderName\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnho5UvugVTX",
        "colab_type": "text"
      },
      "source": [
        "# Merge sites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlnI67nVGmFe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b6d61fc4-ff4a-41d0-dca6-929f46df3857"
      },
      "source": [
        "# create a list of unique sites\n",
        "locs = site.loc[:, [common, 'LatitudeMeasure', 'LongitudeMeasure']]\n",
        "locs.drop_duplicates(inplace=True)\n",
        "length = len(locs)\n",
        "length"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "230"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePo6lw_0HCh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# copy lat / lon to main data\n",
        "old_len = len(d)\n",
        "locs.reset_index(inplace=True, drop=True)\n",
        "d = d.merge(locs.loc[:, [common, 'LatitudeMeasure', 'LongitudeMeasure']], on=[common])\n",
        "assert len(d) == old_len\n",
        "assert sum(d.LatitudeMeasure == 0) == 0\n",
        "assert sum(np.isnan(d.LatitudeMeasure)) == 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7n1oI1XI178",
        "colab_type": "text"
      },
      "source": [
        "Data has different `MonitoringLocationIdentifier`s for the same coords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkFwnJS9IDWN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b1dcdf7d-ec2b-4937-e657-24dd83ec3612"
      },
      "source": [
        "# generate a location based site field\n",
        "lonlat = d[['LongitudeMeasure', 'LatitudeMeasure']].drop_duplicates()\n",
        "dd = geopandas.GeoDataFrame(\n",
        "    lonlat,\n",
        "    geometry=geopandas.points_from_xy(\n",
        "        lonlat.LongitudeMeasure, lonlat.LatitudeMeasure\n",
        "    ),\n",
        "    crs = {'init' :'epsg:4326'},\n",
        ")\n",
        "dd['site'] = dd.LatitudeMeasure.astype(str)+dd.LongitudeMeasure.astype(str)\n",
        "dd.drop(columns=['LongitudeMeasure', 'LatitudeMeasure'], inplace=True)\n",
        "# 50 m buffer merges sites within 100 m of each other\n",
        "dd.geometry = dd.geometry.to_crs(5071).buffer(50)\n",
        "intersect = geopandas.overlay(dd, dd, how='identity')\n",
        "lint = len(intersect)\n",
        "intersect = intersect[intersect.site_2.astype(str) != 'nan']\n",
        "print(\"Dropped %d NaN intersections\" % (lint-len(intersect)))\n",
        "\n",
        "# make a dict of site: (set of intersecting sites)\n",
        "sets = {}\n",
        "for inter in intersect.itertuples():\n",
        "    site_1 = str(inter.site_1)\n",
        "    site_2 = str(inter.site_2)\n",
        "    sets.setdefault(site_1, set()).add(site_2)\n",
        "    sets.setdefault(site_2, set()).add(site_1)\n",
        "# pick the lowest (alphabetically) site as the name for intersecting sites\n",
        "sites = {min(i):i for i in sets.values()}\n",
        "# sites = {(k+\"_X\" if len(v) > 1 else k):v for k, v in sites.items()}\n",
        "[{k:v} for k,v in sites.items() if len(v) > 1]\n",
        "len(lonlat), len(sites)\n",
        "site = {}\n",
        "xtra = {}\n",
        "for k, v in sites.items():\n",
        "    for s in v:\n",
        "        site[s] = k\n",
        "        xtra[s] = len(v) > 1\n",
        "\n",
        "lu = d.loc[:, ['LongitudeMeasure', 'LatitudeMeasure', common]].drop_duplicates(\n",
        "    subset=['LongitudeMeasure', 'LatitudeMeasure'])\n",
        "#X lu = merge(lu, d, ['LongitudeMeasure', 'LatitudeMeasure'], [common])\n",
        "lu['sitehash'] = lu.LatitudeMeasure.astype(str)+lu.LongitudeMeasure.astype(str)\n",
        "sitename = {k:v for k,v in zip(lu.sitehash, lu[common])}\n",
        "lu['site'] = [sitename[site[i]]+('_X' if xtra[i] else '') for i in lu.sitehash]\n",
        "lu.drop_duplicates(inplace=True)\n",
        "\n",
        "d = merge(d, lu, ['LongitudeMeasure', 'LatitudeMeasure'], ['site'])\n",
        "assert len(d) == old_len, (old_len, len(d))   "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dropped 4 NaN intersections\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pXzMa7wJ-3H",
        "colab_type": "text"
      },
      "source": [
        "Now copy site info. into locs table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgY12QBVKEtF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlocs = merge(locs, d, [common, 'LatitudeMeasure', 'LongitudeMeasure'], ['site'])\n",
        "locs_len = len(nlocs)\n",
        "nlocs.drop_duplicates(inplace=True, subset=['site', 'MonitoringLocationIdentifier'])\n",
        "assert len(nlocs) == length, (length, len(nlocs))\n",
        "locs = nlocs\n",
        "locs.to_csv(\"locs.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOItpESyYXtI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for na in '', 'nan', 'NaN':\n",
        "    assert sum(locs['site'] == na) == 0, na\n",
        "    assert sum(d['site'] == na) == 0, na\n",
        "    assert sum(locs['site'] == na+na) == 0, na\n",
        "    assert sum(d['site'] == na+na) == 0, na"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1stEddL4jqzb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c848a340-f724-4426-ec71-a37c09ab4c5d"
      },
      "source": [
        "print(\"%d locations\" % len(locs))\n",
        "distinct = locs.set_index([\"LatitudeMeasure\", \"LongitudeMeasure\"])\n",
        "print(\"%d distinct\" % len(distinct.index.unique()))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "230 locations\n",
            "192 distinct\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXRyvPkVghYY",
        "colab_type": "text"
      },
      "source": [
        "# Data to sampling events"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVH5IJXWkhUN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "outputId": "e6e897de-edad-4823-835d-2d8c71d15744"
      },
      "source": [
        "# tabulate units used for results\n",
        "d['ResultSampleFractionText'] = [\n",
        "    i if i != 'nan' else 'NA'\n",
        "    for i in d['ResultSampleFractionText'].astype(str)\n",
        "]\n",
        "\n",
        "res_types = (\n",
        "    d.groupby(\n",
        "        [\n",
        "            'CharacteristicName',\n",
        "            'ResultMeasure_MeasureUnitCode',\n",
        "            'ResultSampleFractionText',\n",
        "        ]\n",
        "    )\n",
        "    .count()\n",
        "    .iloc[:, 0]\n",
        ")\n",
        "print(res_types)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CharacteristicName          ResultMeasure_MeasureUnitCode  ResultSampleFractionText\n",
            "Orthophosphate              mg/l                           Dissolved                     181\n",
            "                                                           NA                             36\n",
            "                                                           Total                         119\n",
            "                            mg/l as P                      Dissolved                   18392\n",
            "                                                           Total                         380\n",
            "                            mg/l asPO4                     Dissolved                   18393\n",
            "Orthophosphate as P         mg/l                           Dissolved                     122\n",
            "                                                           Total                           1\n",
            "Phosphate-phosphorus        mg/l                           Total                         417\n",
            "Phosphate-phosphorus as P   mg/l                           NA                            147\n",
            "                                                           Total                         211\n",
            "Phosphorus                  mg/kg as P                     Bed Sediment                    8\n",
            "                            mg/l                           Dissolved                       1\n",
            "                                                           NA                             40\n",
            "                                                           Total                         318\n",
            "                            mg/l PO4                       Total                         736\n",
            "                            mg/l as P                      Dissolved                    1171\n",
            "                                                           Total                       19901\n",
            "                            ug/l                           NA                              3\n",
            "Stream flow, instantaneous  ft3/s                          NA                           3244\n",
            "                            m3/sec                         NA                           3244\n",
            "Stream flow, mean. daily    ft3/s                          NA                          15321\n",
            "Name: OrganizationIdentifier, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvDaZ5-pkvVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "duration = wq.db.get('duration', [])  # cache expensive calc. in wq.db\n",
        "if not duration:\n",
        "    for row in d.itertuples():\n",
        "        try:\n",
        "            t0 = parse(\n",
        "                \"%s %s\"\n",
        "                % (row.ActivityStartDate, row.ActivityStartTime_Time)\n",
        "            )\n",
        "            t1 = parse(\n",
        "                \"%s %s\" % (row.ActivityEndDate, row.ActivityEndTime_Time)\n",
        "            )\n",
        "            duration.append((t1 - t0).seconds)\n",
        "        except ValueError:\n",
        "            duration.append(0)\n",
        "    wq.db['duration'] = duration\n",
        "d['duration'] = duration"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIbZBmsClx1c",
        "colab_type": "text"
      },
      "source": [
        "Create an ID field for each sampling event"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYZXRm5jlp-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d['sampling'] = (\n",
        "    d['ActivityStartTime_Time'].astype(str)\n",
        "    + d['ActivityEndTime_Time'].astype(str)\n",
        "    + ' '\n",
        "    + d['ActivityStartDate'].astype(str)\n",
        "    + ' '\n",
        "    + d['ActivityEndDate'].astype(str)\n",
        "    + ' '\n",
        "    + d['LatitudeMeasure'].astype(str)\n",
        "    + ' '\n",
        "    + d['LongitudeMeasure'].astype(str)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Wv941RAmIeb",
        "colab_type": "text"
      },
      "source": [
        "## Save output data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLjS3EC3mMfL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d.to_csv('data.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PqLby2umSla",
        "colab_type": "text"
      },
      "source": [
        "Now we want to match P conc. data and flow data for each sampling event\n",
        "\n",
        "(start to refer to `d` as `data` from here on)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHhBDk2nmh0q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0c8edb94-a6d8-4c43-89b9-9a618c2e572e"
      },
      "source": [
        "data = pd.read_csv(\"data.csv\", low_memory=False)\n",
        "pdata = data.loc[:, ['sampling', 'ActivityStartDate', 'ActivityEndDate', 'site']]\n",
        "for na in '', 'nan', 'NaN':\n",
        "    assert sum(locs['site'] == na) == 0, na\n",
        "    assert sum(d['site'] == na) == 0, na\n",
        "    assert sum(pdata['site'] == na) == 0, na\n",
        "    assert sum(locs['site'] == na+na) == 0, na\n",
        "    assert sum(d['site'] == na+na) == 0, na\n",
        "    assert sum(pdata['site'] == na+na) == 0, na\n",
        "pdata.drop_duplicates(subset=['sampling'], inplace=True)\n",
        "pdata_len = len(pdata)\n",
        "print(\"%d into %d\" % (len(data), pdata_len))\n",
        "cname = 'CharacteristicName'\n",
        "cunit = 'ResultMeasure_MeasureUnitCode'\n",
        "vname = 'ResultMeasureValue'\n",
        "TP = namedtuple(\"ToProcess\", \"to cname unit trans\")\n",
        "po4_to_p = (30.97 + 4 * 16) / 30.97\n",
        "to_process = [\n",
        "    TP('flow', 'Stream flow, instantaneous', 'ft3/s', None),\n",
        "    TP(\n",
        "        'flow',\n",
        "        'Stream flow, instantaneous',\n",
        "        'm3/sec',\n",
        "        lambda x: x * 35.3147,\n",
        "    ),\n",
        "    TP('flow', 'Stream flow, mean. daily', 'ft3/s', None),\n",
        "    TP('conc', 'Phosphorus', 'mg/l as P', None),\n",
        "    TP('conc', 'Phosphorus', 'mg/l', None),\n",
        "    TP('conc', 'Phosphorus', 'ug/l', lambda x: 1000 * x),\n",
        "    TP('conc', 'Phosphorus', 'mg/l PO4', lambda x: x / po4_to_p),\n",
        "    TP('conc', 'Phosphate-phosphorus as P', 'mg/l', None),\n",
        "    TP('conc', 'Phosphate-phosphorus', 'mg/l', None),\n",
        "    TP('conc', 'Orthophosphate as P', 'mg/l', None),\n",
        "    TP('conc', 'Orthophosphate', 'mg/l as P', None),\n",
        "    TP('conc', 'Orthophosphate', 'mg/l asPO4', lambda x: x / po4_to_p),\n",
        "    TP('conc', 'Orthophosphate', 'mg/l', lambda x: x / po4_to_p),\n",
        "    # exclude as it's in bed sediment\n",
        "    # TP('conc', 'Phosphorus', 'mg/kg as P', None),\n",
        "]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "82386 into 22063\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPGAFGYinYDL",
        "colab_type": "text"
      },
      "source": [
        "We're merging 82386 observations into 21877 flow + conc. records.  In a perfect world we'd have one flow and one conc. record for each sampling, but we have both redundant and mismatched (flow without conc. and visa versa) records, roughly 4:1 instead of 2:1.\n",
        "\n",
        "`to_process` is an **ordered** list of items where each item reocords the field we're trying to fill (`to`), the `CharacteristicName` (`cname`) of our prefered source field, our prefered `unit`, and any conversion (`trans`) necessary to use this source field / unit combination.\n",
        "\n",
        "Now we're going to fill the `to` fields (`flow`, `conc`) with the values selected using the list items, filling as many records with the first set of selected data, and only filling missing records with the next (less desireable) list item."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eb6WyR0ozty",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "outputId": "a722253f-91cd-49dc-ca80-53c15e97c090"
      },
      "source": [
        "selected = 0\n",
        "data.set_index('sampling', drop=False, inplace=True)\n",
        "pdata.set_index('sampling', drop=False, inplace=True)\n",
        "for tp_i, tp in enumerate(to_process):\n",
        "    select = np.logical_and(\n",
        "        data[cname] == tp.cname, data[cunit] == tp.unit\n",
        "    )\n",
        "    selected += sum(select)\n",
        "    print(\n",
        "        \"%d Selected %d %s %s (%d)\"\n",
        "        % (\n",
        "            tp_i, sum(select),\n",
        "            tp.cname,\n",
        "            tp.unit,\n",
        "            len(data['sampling'][select].unique()),\n",
        "        )\n",
        "    )\n",
        "    mean = data.loc[select, :]\n",
        "    mean = mean.groupby(level=0).mean()\n",
        "    pdata = pdata.join(mean.loc[:, vname])\n",
        "    assert len(pdata) == pdata_len, (pdata_len, len(pdata))\n",
        "    if tp.trans is not None:\n",
        "        pdata[vname] = pdata[vname].map(tp.trans)\n",
        "\n",
        "    def missing(x, f):\n",
        "        if f not in x.columns:\n",
        "            return np.ones(len(x)).astype(bool)\n",
        "        x = x[f].astype(str)\n",
        "        ans = np.logical_or(np.equal(x, 'None'), np.equal(x, ''))\n",
        "        ans = np.logical_or(ans, np.equal(x, 'nan'))\n",
        "        ans = np.logical_or(ans, np.equal(x, 'NaN'))\n",
        "        return ans\n",
        "\n",
        "    missing0 = missing(pdata, tp.to)\n",
        "    if tp.to not in pdata.columns:\n",
        "        pdata.rename(columns={vname: tp.to}, inplace=True)\n",
        "    else:\n",
        "        # print(len(missing), sum(missing), missing.shape)\n",
        "        #X pdata[tp.to][missing0] = pdata[vname][missing0]\n",
        "        pdata.loc[missing0, tp.to] = pdata[vname][missing0]\n",
        "        pdata.drop(columns=vname, inplace=True)\n",
        "    missing1 = missing(pdata, tp.to)\n",
        "    print(\n",
        "        \"%d, Needed %d, used %d, still need %d\"\n",
        "        % (tp_i, sum(missing0), sum(missing0) - sum(missing1), sum(missing1))\n",
        "    )\n",
        "    assert len(pdata) == pdata_len, (pdata_len, len(pdata))\n",
        "print(\"Selected %s total\" % selected)\n",
        "pdata = pdata[~np.isnan(pdata['conc'])]\n",
        "pdata = pdata[~np.isnan(pdata['flow'])]\n",
        "print(\"Lost %d to blank data\" % (pdata_len-len(pdata)))\n",
        "pdata.to_csv(\"pdata.csv\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Selected 3244 Stream flow, instantaneous ft3/s (3237)\n",
            "0, Needed 22063, used 3237, still need 18826\n",
            "1 Selected 3244 Stream flow, instantaneous m3/sec (3237)\n",
            "1, Needed 18826, used 0, still need 18826\n",
            "2 Selected 15321 Stream flow, mean. daily ft3/s (15283)\n",
            "2, Needed 18826, used 15184, still need 3642\n",
            "3 Selected 21072 Phosphorus mg/l as P (19839)\n",
            "3, Needed 22063, used 19839, still need 2224\n",
            "4 Selected 359 Phosphorus mg/l (307)\n",
            "4, Needed 2224, used 303, still need 1921\n",
            "5 Selected 3 Phosphorus ug/l (3)\n",
            "5, Needed 1921, used 3, still need 1918\n",
            "6 Selected 736 Phosphorus mg/l PO4 (735)\n",
            "6, Needed 1918, used 128, still need 1790\n",
            "7 Selected 358 Phosphate-phosphorus as P mg/l (358)\n",
            "7, Needed 1790, used 358, still need 1432\n",
            "8 Selected 417 Phosphate-phosphorus mg/l (417)\n",
            "8, Needed 1432, used 5, still need 1427\n",
            "9 Selected 123 Orthophosphate as P mg/l (123)\n",
            "9, Needed 1427, used 0, still need 1427\n",
            "10 Selected 18772 Orthophosphate mg/l as P (18345)\n",
            "10, Needed 1427, used 351, still need 1076\n",
            "11 Selected 18393 Orthophosphate mg/l asPO4 (18294)\n",
            "11, Needed 1076, used 0, still need 1076\n",
            "12 Selected 336 Orthophosphate mg/l (288)\n",
            "12, Needed 1076, used 1, still need 1075\n",
            "Selected 82378 total\n",
            "Lost 4711 to blank data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJs7_I32Arpu",
        "colab_type": "text"
      },
      "source": [
        "Of 22,000 sampling events sampling either flow or P, we can't find flow *and* P for 4700.  Most (~3600) of the missing data is flow, so we *could* fill in with flow data, not doing that currently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_kz258y4_mY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# update locs with the number of obs. for each site\n",
        "count = pdata.groupby(by=['site']).count()\n",
        "count.rename(columns={count.columns[0]:'count'}, inplace=True)\n",
        "locs = merge(locs, count, ['site'], ['count'])\n",
        "\n",
        "# as above for min/max date\n",
        "count = pdata.groupby(by=['site']).min()\n",
        "locs = merge(locs, count, ['site'], ['ActivityStartDate'])\n",
        "locs.rename(columns={'ActivityStartDate': 'StartDate'}, inplace=True)\n",
        "count = pdata.groupby(by=['site']).max()\n",
        "# yes, use ActivityStartDate both times\n",
        "locs = merge(locs, count, ['site'], ['ActivityStartDate'])\n",
        "locs.rename(columns={'ActivityStartDate': 'EndDate'}, inplace=True)\n",
        "\n",
        "locs.to_csv(\"locs.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNVpWpYugpiW",
        "colab_type": "text"
      },
      "source": [
        "# Link to flow data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuBkoVLryzqb",
        "colab_type": "text"
      },
      "source": [
        "Now we need flow data for places without conc. data.  Doesn't seem to be in the WQ portal, not strictly speaking a WQ parameter (well, it's Water Quantity, not Water Quality).\n",
        "\n",
        "Ran //waterservices.usgs.gov/nwis/site/?format=rdb&bBox=-80.040200,43.064600,-75.969700,44.355000&seriesCatalogOutput=true&siteType=ST&siteStatus=all and created `waterservices.usgs.gov.txt`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ivmBHjHzvOU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "b084a94a-3184-411a-aa32-fb7a87a01f48"
      },
      "source": [
        "floc = pd.read_csv(\n",
        "    \"waterservices.usgs.gov.txt\", sep=r'\\t', comment='#', engine='python', \n",
        "    skiprows=[43],  # the FORTRANesque column widths\n",
        "    dtype={'site_no': str}\n",
        ")\n",
        "print(\"Total\", len(floc))\n",
        "floc.drop_duplicates(subset=['dec_lat_va', 'dec_long_va', 'agency_cd', 'site_no'], inplace=True)\n",
        "print(\"Unique\", len(floc))\n",
        "floc = floc[~np.isnan(floc.dec_lat_va)]\n",
        "floc = floc[~np.isnan(floc.dec_long_va)]\n",
        "print(\"Non-null\", len(floc))\n",
        "floc.to_csv(\"locs2.csv\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total 28901\n",
            "Unique 322\n",
            "Non-null 321\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAODZpBXvbOK",
        "colab_type": "text"
      },
      "source": [
        "The P sampling `locs` have their original lat/lon values, no necessarily consistent within a site.  That's ok, use these for intersect as it will give all possible answers for each site, we can resolve mulitples later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UAswFxzwcHd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "outputId": "5ec0a224-25b5-4d28-f85a-9d09327bc12f"
      },
      "source": [
        "import folium\n",
        "mdnmap = folium.Map(location=[43.234853, -77.533949], zoom_start=19)\n",
        "\n",
        "for row in locs.itertuples():\n",
        "    folium.Marker([row.LatitudeMeasure, row.LongitudeMeasure], popup=row.site).add_to(mdnmap)\n",
        "mdnmap\n",
        "\n",
        "# ---\n",
        "locblobs = geopandas.GeoDataFrame(\n",
        "    locs,\n",
        "    geometry=geopandas.points_from_xy(\n",
        "        locs.LongitudeMeasure, locs.LatitudeMeasure\n",
        "    ),\n",
        "    crs = {'init' :'epsg:4326'},\n",
        ")\n",
        "locblobs.geometry = locblobs.geometry.to_crs(5071).buffer(50)\n",
        "\n",
        "# don't do this because we might want to use an up/down stream guage for\n",
        "# an unguaged location with lots of P obs.\n",
        "# although this doesn't entirely make sense as we're filtering on the presence\n",
        "# of at least 1 WQ sample of some kind...\n",
        "#\n",
        "# print(\"From\", len(locblobs), \"locations\")\n",
        "# locblobs = locblobs[~np.isnan(locblobs['count'])]\n",
        "# print(\"Go to\", len(locblobs), \"with P obs.\")\n",
        "\n",
        "flocblobs = geopandas.GeoDataFrame(\n",
        "    floc,\n",
        "    geometry=geopandas.points_from_xy(\n",
        "        floc.dec_long_va, floc.dec_lat_va\n",
        "    ),\n",
        "    crs = {'init' :'epsg:4326'},\n",
        ")\n",
        "flocblobs.geometry = flocblobs.geometry.to_crs(5071).buffer(50)\n",
        "# flocblobs['site_id'] = flocblobs.agency_cd + '-' + flocblobs.site_no\n",
        "# because geopandas.overlay seems to change site_no to float\n",
        "\n",
        "intersect = geopandas.overlay(locblobs, flocblobs, how='identity')\n",
        "\n",
        "floc.site_no.dtype\n",
        "flocblobs.site_no.dtype\n",
        "intersect.site_no.dtype\n",
        "# intersect.site_id.dtype\n",
        "\n",
        "\n",
        "lint = len(intersect)\n",
        "intersect = intersect[intersect.site_no.astype(str) != 'nan']\n",
        "print(\"Dropped %d NaN intersections\" % (lint-len(intersect)))\n",
        "\n",
        "# make a dict of site: (set of intersecting flow data)\n",
        "sets = {}\n",
        "for inter in intersect.itertuples():\n",
        "    site_1 = str(inter.site)\n",
        "    site_2 = \"%s-%s\" % (inter.agency_cd, inter.site_no)\n",
        "    sets.setdefault(site_1, set()).add(site_2)\n",
        "\n",
        "[{k:v} for k,v in sets.items() if len(v) > 1]\n",
        "toget = set()\n",
        "for flows in sets.values():\n",
        "    toget.update(flows)\n",
        "\n",
        "tsv_tmpl = {\n",
        "    \"site_no\": 431510077363501,\n",
        "    \"agency_cd\": \"USGS\",\n",
        "    \"inventory_output\": \"0\",\n",
        "    \"rdb_inventory_output\": \"file\",\n",
        "    \"begin_date\": \"2000-01-01\",\n",
        "    \"end_date\": \"2019-07-11\",\n",
        "    \"TZoutput\": \"0\",\n",
        "    \"pm_cd_compare\": \"Greater than\",\n",
        "    \"radio_parm_cds\": \"all_parm_cds\",\n",
        "    \"qw_attributes\": \"0\",\n",
        "    \"format\": \"rdb\",\n",
        "    \"qw_sample_wide\": \"wide\",\n",
        "    \"rdb_qw_attributes\": \"0\",\n",
        "    \"date_format\": \"YYYY-MM-DD\",\n",
        "    \"rdb_compression\": \"gz\",\n",
        "    \"submitted_form\": \"brief_list\",\n",
        "}\n",
        "    \n",
        "    \n",
        "flows = {}    \n",
        "for flow in toget:\n",
        "    flow = flow.replace(\"USGS-\", \"\")\n",
        "    # if flow != \"04249000\":\n",
        "    #     continue\n",
        "    query = dict(\n",
        "        format=\"json\",\n",
        "        indent=\"off\",\n",
        "        sites=flow,\n",
        "        startDT=\"2000-01-01\",\n",
        "        endDT=\"2019-07-10\",\n",
        "        parameterCd=\"00060\",  # ft3/s\n",
        "    )\n",
        "    jsonfile = wq.get_data(query, 'flow')\n",
        "    print(WQ_API_URL['flow']+'?'+'&'.join(\"%s=%s\" % (k, v) for k, v in query.items() if not k.startswith('_')))\n",
        "    data = json.load(open(jsonfile))\n",
        "    data = data['value']['timeSeries']\n",
        "    if data:\n",
        "        data = data[0]['values']\n",
        "        assert len(data) == 1, len(data)\n",
        "        data = data[0]['value']\n",
        "        print(flow, len(data))\n",
        "        flows[flow] = [(i['dateTime'], float(i['value'])) for i in data]\n",
        "    else:\n",
        "        # fall back to TSV data\n",
        "        query = tsv_tmpl.copy()\n",
        "        query['site_no'] = str(flow)\n",
        "        data = pd.read_csv(wq.get_data(query, 'flowTSV'))        \n",
        "\n",
        "# flows['431510077363501']"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dropped 161 NaN intersections\n",
            "https://waterservices.usgs.gov/nwis/dv/?format=json&indent=off&sites=04218054&startDT=2000-01-01&endDT=2019-07-10&parameterCd=00060\n",
            "Query 34c2f20583fdda48e33a1ba473cc05730bcea25dab5c77408e24ceed3f4cce1c\n",
            "use_get True\n",
            "{\n",
            "  \"TZoutput\": \"0\",\n",
            "  \"_api\": \"https://nwis.waterdata.usgs.gov/usa/nwis/qwdata/\",\n",
            "  \"agency_cd\": \"USGS\",\n",
            "  \"begin_date\": \"2000-01-01\",\n",
            "  \"date_format\": \"YYYY-MM-DD\",\n",
            "  \"end_date\": \"2019-07-11\",\n",
            "  \"format\": \"rdb\",\n",
            "  \"inventory_output\": \"0\",\n",
            "  \"pm_cd_compare\": \"Greater than\",\n",
            "  \"qw_attributes\": \"0\",\n",
            "  \"qw_sample_wide\": \"wide\",\n",
            "  \"radio_parm_cds\": \"all_parm_cds\",\n",
            "  \"rdb_compression\": \"gz\",\n",
            "  \"rdb_inventory_output\": \"file\",\n",
            "  \"rdb_qw_attributes\": \"0\",\n",
            "  \"site_no\": \"04218054\",\n",
            "  \"submitted_form\": \"brief_list\"\n",
            "}\n",
            "Reading data .\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-06751e2e0ef2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtsv_tmpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mquery\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'site_no'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'flowTSV'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;31m# flows['431510077363501']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/My Drive/Colab Notebooks/LOads/get_wqdata.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, query, type_)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0min_\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# skip \"# comment lines\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'#'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/gzip.py\u001b[0m in \u001b[0;36mread1\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT_BUFFER_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpeek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/_compression.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbyte_view\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_view\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mbyte_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    461\u001b[0m                 \u001b[0;31m# jump to the next member, if there is one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_gzip_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/gzip.py\u001b[0m in \u001b[0;36m_read_gzip_header\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmagic\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34mb'\\037\\213'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Not a gzipped file (%r)'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmagic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         (method, flag,\n",
            "\u001b[0;31mOSError\u001b[0m: Not a gzipped file (b'No')"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jMbPGCekgzA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f906c1a2-1b1b-4a24-9b7e-26b4a2de5ec6"
      },
      "source": [
        "\n",
        "locs['sFlow'] = None\n",
        "locs['eFlow'] = None\n",
        "locs['cFlow'] = None\n",
        "for loc in locs.itertuples():\n",
        "    if not loc.count:\n",
        "        continue\n",
        "    if loc.site not in sets:\n",
        "        print(loc.site, \"no flow data source found\")\n",
        "        continue\n",
        "    flow = loc.site.replace(\"USGS-\", \"\")\n",
        "    if flow not in flows:\n",
        "        print(loc.site, \"no flow data found\")\n",
        "        continue\n",
        "    print(\"Populating\", loc.site)\n",
        "    for src in sets[loc.site]:\n",
        "        if loc.sFlow:\n",
        "            print(\"Overwriting data for\", loc.site)\n",
        "        flow = src.replace(\"USGS-\", \"\")\n",
        "        locs.loc[loc.Index, 'sFlow'] = min(i[0] for i in flows[flow])\n",
        "        locs.loc[loc.Index, 'eFlow'] = max(i[0] for i in flows[flow])\n",
        "        locs.loc[loc.Index, 'cFlow'] = len(flows[flow])\n",
        "        # print(len(flows[flow]), 'for', locs.site)\n",
        "locs.to_csv(\"locs.csv\")\n",
        "sets['USGS-431510077363501']\n",
        "# flows['431510077363501']"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "USGS-04217500 no flow data found\n",
            "USGS-04218000_X no flow data found\n",
            "USGS-04218030 no flow data found\n",
            "USGS-04218035 no flow data found\n",
            "USGS-04218054 no flow data found\n",
            "USGS-04218592 no flow data found\n",
            "USGS-04218610 no flow data found\n",
            "USGS-04218700 no flow data found\n",
            "USGS-04218740 no flow data found\n",
            "USGS-04219355 no flow data found\n",
            "USGS-04219501 no flow data found\n",
            "USGS-04219640 no flow data found\n",
            "USGS-04219650 no flow data found\n",
            "USGS-04219720 no flow data found\n",
            "USGS-04219726 no flow data found\n",
            "USGS-04219735 no flow data found\n",
            "USGS-04219765 no flow data found\n",
            "Populating USGS-04219768\n",
            "USGS-04219775 no flow data found\n",
            "USGS-04219915 no flow data found\n",
            "USGS-04219979 no flow data found\n",
            "USGS-04219997 no flow data found\n",
            "USGS-0421999720 no flow data found\n",
            "USGS-0421999750 no flow data found\n",
            "USGS-04219998 no flow data found\n",
            "USGS-04219999 no flow data found\n",
            "USGS-04220045_X no flow data found\n",
            "Populating USGS-0422016550\n",
            "Populating USGS-04220223\n",
            "USGS-04220230 no flow data found\n",
            "USGS-04220252 no flow data found\n",
            "Populating USGS-0422026250\n",
            "USGS-04230650 no flow data found\n",
            "USGS-04230800 no flow data found\n",
            "Populating USGS-04231000\n",
            "USGS-04231050 no flow data found\n",
            "USGS-04231100 no flow data found\n",
            "Populating USGS-04231600\n",
            "USGS-04231700 no flow data found\n",
            "Populating USGS-04232000\n",
            "USGS-04232006 no flow data found\n",
            "USGS-04232042 no flow data found\n",
            "USGS-0423204539_X no flow data found\n",
            "USGS-0423204539_X no flow data found\n",
            "USGS-04232046_X no flow data found\n",
            "USGS-430715077283802_X no flow data found\n",
            "Populating USGS-0423204920\n",
            "Populating USGS-04232050\n",
            "USGS-0423205010_X no flow data found\n",
            "USGS-0423205023_X no flow data found\n",
            "USGS-0423205025_X no flow data found\n",
            "USGS-431405077320500_X no flow data found\n",
            "USGS-04232060 no flow data found\n",
            "USGS-04232070 no flow data found\n",
            "21NYDECA-03022102_X no flow data found\n",
            "Populating USGS-04232100\n",
            "USGS-04234270 no flow data found\n",
            "USGS-04234293 no flow data found\n",
            "USGS-04234300 no flow data found\n",
            "USGS-04235293 no flow data found\n",
            "Populating USGS-04235600\n",
            "USGS-04237020 no flow data found\n",
            "USGS-04237410_X no flow data found\n",
            "Populating USGS-04237500\n",
            "USGS-04240108 no flow data found\n",
            "USGS-0424011445 no flow data found\n",
            "Populating USGS-04240120\n",
            "USGS-04240135 no flow data found\n",
            "Populating USGS-04240300\n",
            "USGS-04240500 no flow data found\n",
            "USGS-04245275 no flow data found\n",
            "USGS-04245500 no flow data found\n",
            "Populating USGS-04245840\n",
            "USGS-04246500 no flow data found\n",
            "USGS-04246590 no flow data found\n",
            "USGS-04246601 no flow data found\n",
            "USGS-04247038 no flow data found\n",
            "USGS-04247470 no flow data found\n",
            "USGS-04248200 no flow data found\n",
            "USGS-04248250 no flow data found\n",
            "Populating USGS-04249000\n",
            "USGS-04249050 no flow data found\n",
            "USGS-04249060_X no flow data found\n",
            "USGS-04249062 no flow data found\n",
            "USGS-04249910 no flow data found\n",
            "USGS-04249950 no flow data found\n",
            "USGS-04250000 no flow data found\n",
            "Populating USGS-04250200\n",
            "USGS-04250310 no flow data found\n",
            "USGS-04250380 no flow data found\n",
            "USGS-04250385 no flow data found\n",
            "Populating USGS-0425040001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-a2d119009e7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Overwriting data for\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mflow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"USGS-\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mlocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sFlow'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mflows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mlocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'eFlow'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mflows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mlocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cFlow'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: '04250400'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqgHc1-4yaYz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}