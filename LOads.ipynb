{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LOads.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tbnorth/LOads_colab/blob/master/LOads.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONOS-z05f6dp",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW-Jh5er5kCq",
        "colab_type": "text"
      },
      "source": [
        "```python\n",
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62UIOjgh6s1C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\")\n",
        "ROOT = Path(\"/content/gdrive/My Drive/Colab Notebooks\")\n",
        "BASE = ROOT.joinpath('LOads')\n",
        "BASE.mkdir(exist_ok=True)\n",
        "os.chdir(BASE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyaGn8iZEi3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install libspatialindex-dev python3-rtree\n",
        "!pip install geopandas\n",
        "!ldconfig\n",
        "from rtree import index\n",
        "from rtree.index import Rtree\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas\n",
        "from collections import namedtuple\n",
        "from dateutil.parser import parse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3E2_7srZVvJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def merge(left, right, on, cols):\n",
        "    for col in cols:\n",
        "        if col in left.columns:\n",
        "            left = left.drop(columns=col)\n",
        "    right = right.reset_index()\n",
        "    return left.merge(right[on+cols], on=on, how='left')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wheVzmbh73ny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# uploaded get_wqdata.py to Colab Notebooks folder, now add to import path\n",
        "import sys\n",
        "if BASE not in sys.path:\n",
        "    sys.path.append(str(BASE))\n",
        "from get_wqdata import GetWQData\n",
        "\n",
        "wq = GetWQData()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Mm-pDqcgB0Y",
        "colab_type": "text"
      },
      "source": [
        "# Explore dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cx15z3nOB5uJ",
        "colab_type": "text"
      },
      "source": [
        "Want to work out which CharacteristicName values to request.  The WQ portal web interface says they come [from here](http://iaspub.epa.gov/sor_internet/registry/substreg/home/overview/home.do) but that's hard to search, so request all obs. in our bounding box and search that list instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOM5hZUYCYxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query = dict(\n",
        "    _desc=\"All sample types\",\n",
        "    bBox=\"-80.0402,43.0646,-75.9697,44.355\",\n",
        "    siteType=[\"Aggregate surface-water-use\", \"Stream\"],\n",
        "    sampleMedia=[\"Water\", \"water\"],\n",
        "    # siteid=[\"USGS-04231600\"],\n",
        "    # startDateLo=\"01-01-2018\",\n",
        "    # startDateHi=\"02-01-2018\",\n",
        ")\n",
        "print(\"Reading data\")\n",
        "all_data = wq.get_data(query)  # gets path to .csv file\n",
        "d = pd.read_csv(all_data)  # , nrows=10)\n",
        "print(\"Data read\")\n",
        "print(d.columns)\n",
        "po4 = set(d[\"CharacteristicName\"])\n",
        "keep = set()\n",
        "for term in 'po4', 'phosphor', 'srp', 'phosphate', 'flow', 'guage', 'cfs':\n",
        "    for cname in po4:\n",
        "        if term in cname.lower():\n",
        "            keep.add(cname)\n",
        "print(sorted(keep))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZxLuVimEBVF",
        "colab_type": "text"
      },
      "source": [
        "Now make a sensible list from the values we found."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVO8fq7dBn55",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_cache = {\n",
        "    'characteric_names': [\n",
        "        # 'O-Ethyl O-methyl S-propyl phosphorothioate',\n",
        "        # 'O-Ethyl S-methyl S-propyl phosphorodithioate',\n",
        "        # 'O-Ethyl S-propyl phosphorothioate',\n",
        "        # 'Organic phosphorus',\n",
        "        'Orthophosphate',\n",
        "        'Orthophosphate as P',\n",
        "        'Phosphate-phosphorus',\n",
        "        'Phosphate-phosphorus as P',\n",
        "        'Phosphorus',\n",
        "        'Stream flow, instantaneous',\n",
        "        'Stream flow, mean. daily',\n",
        "        # 'Tributyl phosphate',\n",
        "        # 'Triphenyl phosphate',\n",
        "        # 'Tris(1,3-dichloro-2-propyl)phosphate',\n",
        "        # 'Tris(2-butoxyethyl) phosphate',\n",
        "        # 'Tris(2-chloroethyl) phosphate',\n",
        "    ]\n",
        "}\n",
        "\n",
        "names = use_cache.get('characteric_names')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXAT9ESNFZXc",
        "colab_type": "text"
      },
      "source": [
        "Now grab all the data we're interested in"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLzhWPqJFQ7s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query = dict(\n",
        "    _desc=\"Target sample types\",\n",
        "    bBox=\"-80.0402,43.0646,-75.9697,44.355\",\n",
        "    siteType=[\"Aggregate surface-water-use\", \"Stream\"],\n",
        "    sampleMedia=[\"Water\", \"water\"],\n",
        "    characteristicName=names,\n",
        ")\n",
        "print(\"Reading data\")\n",
        "# first get the Site info. flavored response\n",
        "data = wq.get_data(query, type_='site')  # gets path to .csv file\n",
        "site = pd.read_csv(data)\n",
        "# print('\\n'.join(sorted(site.columns)))\n",
        "# then get the Result info.\n",
        "data = wq.get_data(query, type_='result')\n",
        "d = pd.read_csv(data, low_memory=False)\n",
        "d.columns = [i.replace('/', '_') for i in d.columns]\n",
        "# get rid of \"mg/l<space><space><space>\" units\n",
        "for col in ['ResultMeasure_MeasureUnitCode']:\n",
        "    d[col] = [str(i).strip() for i in d[col]]\n",
        "had = len(d)\n",
        "d = d.loc[d['ResultMeasure_MeasureUnitCode'] != 'nan', :]\n",
        "print(\"Lost %d for missing units\" % (had - len(d)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqUj6aVFGVV2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# find common columns\n",
        "common = set(site.columns).intersection(set(d.columns))\n",
        "print('\\n'.join(sorted(common)))\n",
        "# use this common column\n",
        "common = 'MonitoringLocationIdentifier'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnho5UvugVTX",
        "colab_type": "text"
      },
      "source": [
        "# Merge sites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlnI67nVGmFe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a list of unique sites\n",
        "locs = site.loc[:, [common, 'LatitudeMeasure', 'LongitudeMeasure']]\n",
        "locs.drop_duplicates(inplace=True)\n",
        "length = len(locs)\n",
        "length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePo6lw_0HCh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# copy lat / lon to main data\n",
        "old_len = len(d)\n",
        "locs.reset_index(inplace=True, drop=True)\n",
        "d = d.merge(locs.loc[:, [common, 'LatitudeMeasure', 'LongitudeMeasure']], on=[common])\n",
        "assert len(d) == old_len\n",
        "assert sum(d.LatitudeMeasure == 0) == 0\n",
        "assert sum(np.isnan(d.LatitudeMeasure)) == 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7n1oI1XI178",
        "colab_type": "text"
      },
      "source": [
        "Data has different `MonitoringLocationIdentifier`s for the same coords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkFwnJS9IDWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate a location based site field\n",
        "lonlat = d[['LongitudeMeasure', 'LatitudeMeasure']].drop_duplicates()\n",
        "dd = geopandas.GeoDataFrame(\n",
        "    lonlat,\n",
        "    geometry=geopandas.points_from_xy(\n",
        "        lonlat.LongitudeMeasure, lonlat.LatitudeMeasure\n",
        "    ),\n",
        "    crs = {'init' :'epsg:4326'},\n",
        ")\n",
        "dd['site'] = dd.LatitudeMeasure.astype(str)+dd.LongitudeMeasure.astype(str)\n",
        "dd.drop(columns=['LongitudeMeasure', 'LatitudeMeasure'], inplace=True)\n",
        "# 50 m buffer merges sites within 100 m of each other\n",
        "dd.geometry = dd.geometry.to_crs(5071).buffer(50)\n",
        "intersect = geopandas.overlay(dd, dd, how='identity')\n",
        "lint = len(intersect)\n",
        "intersect = intersect[intersect.site_2.astype(str) != 'nan']\n",
        "print(\"Dropped %d NaN intersections\" % (lint-len(intersect)))\n",
        "\n",
        "# make a dict of site: (set of intersecting sites)\n",
        "sets = {}\n",
        "for inter in intersect.itertuples():\n",
        "    site_1 = str(inter.site_1)\n",
        "    site_2 = str(inter.site_2)\n",
        "    sets.setdefault(site_1, set()).add(site_2)\n",
        "    sets.setdefault(site_2, set()).add(site_1)\n",
        "# pick the lowest (alphabetically) site as the name for intersecting sites\n",
        "sites = {min(i):i for i in sets.values()}\n",
        "# sites = {(k+\"_X\" if len(v) > 1 else k):v for k, v in sites.items()}\n",
        "[{k:v} for k,v in sites.items() if len(v) > 1]\n",
        "len(lonlat), len(sites)\n",
        "site = {}\n",
        "xtra = {}\n",
        "for k, v in sites.items():\n",
        "    for s in v:\n",
        "        site[s] = k\n",
        "        xtra[s] = len(v) > 1\n",
        "\n",
        "lu = d.loc[:, ['LongitudeMeasure', 'LatitudeMeasure', common]].drop_duplicates(\n",
        "    subset=['LongitudeMeasure', 'LatitudeMeasure'])\n",
        "#X lu = merge(lu, d, ['LongitudeMeasure', 'LatitudeMeasure'], [common])\n",
        "lu['sitehash'] = lu.LatitudeMeasure.astype(str)+lu.LongitudeMeasure.astype(str)\n",
        "sitename = {k:v for k,v in zip(lu.sitehash, lu[common])}\n",
        "lu['site'] = [sitename[site[i]]+('_X' if xtra[i] else '') for i in lu.sitehash]\n",
        "lu.drop_duplicates(inplace=True)\n",
        "\n",
        "d = merge(d, lu, ['LongitudeMeasure', 'LatitudeMeasure'], ['site'])\n",
        "assert len(d) == old_len, (old_len, len(d))   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pXzMa7wJ-3H",
        "colab_type": "text"
      },
      "source": [
        "Now copy site info. into locs table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgY12QBVKEtF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlocs = merge(locs, d, [common, 'LatitudeMeasure', 'LongitudeMeasure'], ['site'])\n",
        "locs_len = len(nlocs)\n",
        "nlocs.drop_duplicates(inplace=True, subset=['site', 'MonitoringLocationIdentifier'])\n",
        "assert len(nlocs) == length, (length, len(nlocs))\n",
        "locs = nlocs\n",
        "locs.to_csv(\"locs.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOItpESyYXtI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for na in '', 'nan', 'NaN':\n",
        "    assert sum(locs['site'] == na) == 0, na\n",
        "    assert sum(d['site'] == na) == 0, na\n",
        "    assert sum(locs['site'] == na+na) == 0, na\n",
        "    assert sum(d['site'] == na+na) == 0, na"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1stEddL4jqzb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"%d locations\" % len(locs))\n",
        "distinct = locs.set_index([\"LatitudeMeasure\", \"LongitudeMeasure\"])\n",
        "print(\"%d distinct\" % len(distinct.index.unique()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXRyvPkVghYY",
        "colab_type": "text"
      },
      "source": [
        "# Data to sampling events"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVH5IJXWkhUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tabulate units used for results\n",
        "d['ResultSampleFractionText'] = [\n",
        "    i if i != 'nan' else 'NA'\n",
        "    for i in d['ResultSampleFractionText'].astype(str)\n",
        "]\n",
        "\n",
        "res_types = (\n",
        "    d.groupby(\n",
        "        [\n",
        "            'CharacteristicName',\n",
        "            'ResultMeasure_MeasureUnitCode',\n",
        "            'ResultSampleFractionText',\n",
        "        ]\n",
        "    )\n",
        "    .count()\n",
        "    .iloc[:, 0]\n",
        ")\n",
        "print(res_types)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvDaZ5-pkvVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "duration = wq.db.get('duration', [])  # cache expensive calc. in wq.db\n",
        "if not duration:\n",
        "    for row in d.itertuples():\n",
        "        try:\n",
        "            t0 = parse(\n",
        "                \"%s %s\"\n",
        "                % (row.ActivityStartDate, row.ActivityStartTime_Time)\n",
        "            )\n",
        "            t1 = parse(\n",
        "                \"%s %s\" % (row.ActivityEndDate, row.ActivityEndTime_Time)\n",
        "            )\n",
        "            duration.append((t1 - t0).seconds)\n",
        "        except ValueError:\n",
        "            duration.append(0)\n",
        "    wq.db['duration'] = duration\n",
        "d['duration'] = duration"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIbZBmsClx1c",
        "colab_type": "text"
      },
      "source": [
        "Create an ID field for each sampling event"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYZXRm5jlp-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d['sampling'] = (\n",
        "    d['ActivityStartTime_Time'].astype(str)\n",
        "    + d['ActivityEndTime_Time'].astype(str)\n",
        "    + ' '\n",
        "    + d['ActivityStartDate'].astype(str)\n",
        "    + ' '\n",
        "    + d['ActivityEndDate'].astype(str)\n",
        "    + ' '\n",
        "    + d['LatitudeMeasure'].astype(str)\n",
        "    + ' '\n",
        "    + d['LongitudeMeasure'].astype(str)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Wv941RAmIeb",
        "colab_type": "text"
      },
      "source": [
        "## Save output data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLjS3EC3mMfL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d.to_csv('data.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PqLby2umSla",
        "colab_type": "text"
      },
      "source": [
        "Now we want to match P conc. data and flow data for each sampling event\n",
        "\n",
        "(start to refer to `d` as `data` from here on)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHhBDk2nmh0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(\"data.csv\", low_memory=False)\n",
        "pdata = data.loc[:, ['sampling', 'ActivityStartDate', 'ActivityEndDate', 'site']]\n",
        "for na in '', 'nan', 'NaN':\n",
        "    assert sum(locs['site'] == na) == 0, na\n",
        "    assert sum(d['site'] == na) == 0, na\n",
        "    assert sum(pdata['site'] == na) == 0, na\n",
        "    assert sum(locs['site'] == na+na) == 0, na\n",
        "    assert sum(d['site'] == na+na) == 0, na\n",
        "    assert sum(pdata['site'] == na+na) == 0, na\n",
        "pdata.drop_duplicates(subset=['sampling'], inplace=True)\n",
        "pdata_len = len(pdata)\n",
        "print(\"%d into %d\" % (len(data), pdata_len))\n",
        "cname = 'CharacteristicName'\n",
        "cunit = 'ResultMeasure_MeasureUnitCode'\n",
        "vname = 'ResultMeasureValue'\n",
        "TP = namedtuple(\"ToProcess\", \"to cname unit trans\")\n",
        "po4_to_p = (30.97 + 4 * 16) / 30.97\n",
        "to_process = [\n",
        "    TP('flow', 'Stream flow, instantaneous', 'ft3/s', None),\n",
        "    TP(\n",
        "        'flow',\n",
        "        'Stream flow, instantaneous',\n",
        "        'm3/sec',\n",
        "        lambda x: x * 35.3147,\n",
        "    ),\n",
        "    TP('flow', 'Stream flow, mean. daily', 'ft3/s', None),\n",
        "    TP('conc', 'Phosphorus', 'mg/l as P', None),\n",
        "    TP('conc', 'Phosphorus', 'mg/l', None),\n",
        "    TP('conc', 'Phosphorus', 'ug/l', lambda x: 1000 * x),\n",
        "    TP('conc', 'Phosphorus', 'mg/l PO4', lambda x: x / po4_to_p),\n",
        "    TP('conc', 'Phosphate-phosphorus as P', 'mg/l', None),\n",
        "    TP('conc', 'Phosphate-phosphorus', 'mg/l', None),\n",
        "    TP('conc', 'Orthophosphate as P', 'mg/l', None),\n",
        "    TP('conc', 'Orthophosphate', 'mg/l as P', None),\n",
        "    TP('conc', 'Orthophosphate', 'mg/l asPO4', lambda x: x / po4_to_p),\n",
        "    TP('conc', 'Orthophosphate', 'mg/l', lambda x: x / po4_to_p),\n",
        "    # exclude as it's in bed sediment\n",
        "    # TP('conc', 'Phosphorus', 'mg/kg as P', None),\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPGAFGYinYDL",
        "colab_type": "text"
      },
      "source": [
        "We're merging 82386 observations into 21877 flow + conc. records.  In a perfect world we'd have one flow and one conc. record for each sampling, but we have both redundant and mismatched (flow without conc. and visa versa) records, roughly 4:1 instead of 2:1.\n",
        "\n",
        "`to_process` is an **ordered** list of items where each item reocords the field we're trying to fill (`to`), the `CharacteristicName` (`cname`) of our prefered source field, our prefered `unit`, and any conversion (`trans`) necessary to use this source field / unit combination.\n",
        "\n",
        "Now we're going to fill the `to` fields (`flow`, `conc`) with the values selected using the list items, filling as many records with the first set of selected data, and only filling missing records with the next (less desireable) list item."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eb6WyR0ozty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "selected = 0\n",
        "data.set_index('sampling', drop=False, inplace=True)\n",
        "pdata.set_index('sampling', drop=False, inplace=True)\n",
        "for tp_i, tp in enumerate(to_process):\n",
        "    select = np.logical_and(\n",
        "        data[cname] == tp.cname, data[cunit] == tp.unit\n",
        "    )\n",
        "    selected += sum(select)\n",
        "    print(\n",
        "        \"%d Selected %d %s %s (%d)\"\n",
        "        % (\n",
        "            tp_i, sum(select),\n",
        "            tp.cname,\n",
        "            tp.unit,\n",
        "            len(data['sampling'][select].unique()),\n",
        "        )\n",
        "    )\n",
        "    mean = data.loc[select, :]\n",
        "    mean = mean.groupby(level=0).mean()\n",
        "    pdata = pdata.join(mean.loc[:, vname])\n",
        "    assert len(pdata) == pdata_len, (pdata_len, len(pdata))\n",
        "    if tp.trans is not None:\n",
        "        pdata[vname] = pdata[vname].map(tp.trans)\n",
        "\n",
        "    def missing(x, f):\n",
        "        if f not in x.columns:\n",
        "            return np.ones(len(x)).astype(bool)\n",
        "        x = x[f].astype(str)\n",
        "        ans = np.logical_or(np.equal(x, 'None'), np.equal(x, ''))\n",
        "        ans = np.logical_or(ans, np.equal(x, 'nan'))\n",
        "        ans = np.logical_or(ans, np.equal(x, 'NaN'))\n",
        "        return ans\n",
        "\n",
        "    missing0 = missing(pdata, tp.to)\n",
        "    if tp.to not in pdata.columns:\n",
        "        pdata.rename(columns={vname: tp.to}, inplace=True)\n",
        "    else:\n",
        "        # print(len(missing), sum(missing), missing.shape)\n",
        "        #X pdata[tp.to][missing0] = pdata[vname][missing0]\n",
        "        pdata.loc[missing0, tp.to] = pdata[vname][missing0]\n",
        "        pdata.drop(columns=vname, inplace=True)\n",
        "    missing1 = missing(pdata, tp.to)\n",
        "    print(\n",
        "        \"%d, Needed %d, used %d, still need %d\"\n",
        "        % (tp_i, sum(missing0), sum(missing0) - sum(missing1), sum(missing1))\n",
        "    )\n",
        "    assert len(pdata) == pdata_len, (pdata_len, len(pdata))\n",
        "print(\"Selected %s total\" % selected)\n",
        "pdata = pdata[~np.isnan(pdata['conc'])]\n",
        "pdata = pdata[~np.isnan(pdata['flow'])]\n",
        "print(\"Lost %d to blank data\" % (pdata_len-len(pdata)))\n",
        "pdata.to_csv(\"pdata.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJs7_I32Arpu",
        "colab_type": "text"
      },
      "source": [
        "Of 22,000 sampling events sampling either flow or P, we can't find flow *and* P for 4700.  Most (~3600) of the missing data is flow, so we *could* fill in with flow data, not doing that currently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_kz258y4_mY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# update locs with the number of obs. for each site\n",
        "count = pdata.groupby(by=['site']).count()\n",
        "count.rename(columns={count.columns[0]:'count'}, inplace=True)\n",
        "locs = merge(locs, count, ['site'], ['count'])\n",
        "\n",
        "# as above for min/max date\n",
        "count = pdata.groupby(by=['site']).min()\n",
        "locs = merge(locs, count, ['site'], ['ActivityStartDate'])\n",
        "locs.rename(columns={'ActivityStartDate': 'StartDate'}, inplace=True)\n",
        "count = pdata.groupby(by=['site']).max()\n",
        "# yes, use ActivityStartDate both times\n",
        "locs = merge(locs, count, ['site'], ['ActivityStartDate'])\n",
        "locs.rename(columns={'ActivityStartDate': 'EndDate'}, inplace=True)\n",
        "\n",
        "locs.to_csv(\"locs.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNVpWpYugpiW",
        "colab_type": "text"
      },
      "source": [
        "# Link to flow data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuBkoVLryzqb",
        "colab_type": "text"
      },
      "source": [
        "Now we need flow data for places without conc. data.  Doesn't seem to be in the WQ portal, not strictly speaking a WQ parameter (well, it's Water Quantity, not Water Quality).\n",
        "\n",
        "Ran //waterservices.usgs.gov/nwis/site/?format=rdb&bBox=-80.040200,43.064600,-75.969700,44.355000&seriesCatalogOutput=true&siteType=ST&siteStatus=all and created `waterservices.usgs.gov.txt`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ivmBHjHzvOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d = pd.read_csv(\n",
        "    \"waterservices.usgs.gov.txt\", sep=r'\\t', comment='#', engine='python'\n",
        ")\n",
        "print(len(d))\n",
        "d.set_index(['dec_lat_va', 'dec_long_va'], inplace=True, drop=False)\n",
        "print(len(d.index.unique()))\n",
        "d.groupby(level=[0, 1]).min().to_csv(\"locs2.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}